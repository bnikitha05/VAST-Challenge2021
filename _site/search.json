{
  "articles": [
    {
      "path": "answers.html",
      "title": "4.0 Response to Questions",
      "author": [],
      "contents": "\r\n\r\nContents\r\n4.1 Characterization of News Data Sources4.1.1 Correlated Articles\r\n4.1.2 Phrases Occuring Together\r\n\r\n4.2 Biases identified in the News Sources4.2.1 Setting the Scene\r\n4.2.2 Content across Clusters\r\n4.2.3 Comparing Titles\r\n4.2.4 Frequency of keywords in Titles\r\n4.2.5 Timeline: Unfolding of Events across Years\r\n\r\n4.3 Exploring Relationships4.3.1 Overall Distribution of Emails\r\n4.3.2 Deeper Look into Sensitive Emails\r\n4.3.3 Citizenship Biases in Employment Type\r\n4.3.4 Most Important Employees\r\n\r\n\r\n4.1 Characterization of News Data Sources\r\n4.1.1 Correlated Articles\r\nThe correlation graph shows that there are certain newsgroups highly correlated to each other with a value as much as 0.9. Two of the most correlated newsgroups are the “The World” and “Who What News”. These are the nodes that has the thickest edge connecting them and the thicker the line, the more similar words these two newsgroups have in common. The wordclouds for the two mentioned newsgroups give a more deeper understanding of the words that are similar and it appears that both of these published articles mostly containing the words “gastech”, “kronos”, “government”, “contamination” and “environment”. From this, we can understand that these news groups voiced out about the problems created by the GasTech company.\r\n\r\n\r\n4.1.2 Phrases Occuring Together\r\nFrom the bigram plot below, we can understand the usage a a few words. For example, “sten” and “sanjorge” individually is difficult to understand what it means but from the bigram notation, we can understand that it is referring to the CEO of GasTech Jr Sten Sanjorge. Another interesting phrase is ‘abila police force’ and ‘20 jan 2014’ which specifies the day of the incident.\r\nOne notable observation from this plot is the connection between “kronos”, “government” and “pok”. This connection shows that both the government and pok were equally referred to across all articles and this can be extracted from the darkness of the arrow which is same for “kronos -> government” and “kronos -> pok”. Also, the term ‘gas’ was not only used to specify the fields and drilling of the GasTech company but also shows so traces of “tear gas” which the police could have used against the protesters.\r\n\r\n\r\n4.2 Biases identified in the News Sources\r\n4.2.1 Setting the Scene\r\nThe newsgroups are divided into 6 clusters based on the their use of the same words. These clusters can be identified with the different color schemes and the thicker the edge between two nodes suggests that the nodes are more similar.\r\nFrom the text plot, it is visible that cluster number 2 consisting of “Tethys News”, “odern Rubicon”, “Centrum Sentinel” and “Kronos Star” have more number of similar words as the edges connecting them are darker.\r\n\r\n\r\n4.2.2 Content across Clusters\r\nThe content across all articles in cluster 1 seems to be related to the leader of POK Elian Karel, the government and the police. There also are some traces of water most likely representing the water contamination that is occurring. In the whole, this cluster talks more about the relationship between the police, government and Elian Karel with very little involvement of GasTech company.\r\nOn the other hand, cluster 2 focuses more on the POK and avoids targeting any one member of the group. This cluster also takes about the employees withing GasTech which was not seen in cluster 1. Moving on to cluster 3, this is quite similar to cluster 1 other than the fact that ther is not mention of Elian Karel in this cluster.\r\n\r\n\r\nThe articles in cluster 4 are the only ones that have talked about GAStech CEO Sten Sanjorge, Jr. There is also a strong relation between “gastech”, “international” and “kronos” which could suggest that the CEO has brought up talks about investing in international lad potentially being Kronos. Cluster 5 in general is categorized into two aspect. One talks about the type of company GasTeach is and this can be understood by the relation between “oil”, “energy”, “tethys” and “gas”. The other category talks about the relation between the government and POK.\r\nThe last and final cluster mainly focuses on the tension between the police and POK just like in cluster 2 however, there are also some minor article written about health and this is the only cluster that mentioned this.\r\n\r\n\r\n4.2.3 Comparing Titles\r\nFrom the wordcloud below, it is visible that “News Online Today” is the newsgroup that mostly talked about the situation in Kronos.It talked mostly about POK but on the contrast, very little was spoken about the contamination and health of the villagers. Due this, it appears that the newsgroup is biased by targeting the POK group the most. On the other hand, “The Continent” seems to have reported most articles on the calamity, evacuation and discharge but no keywords resemble the GasTech nor the POK. “Everyday News” talked more on the crashes and explode that were occurring in Kronos. The remaining 3 newsgroups appear to be international groups and their keywords are vary diverse and none specifically talking about the situation in Kronos.\r\n\r\n\r\n4.2.4 Frequency of keywords in Titles\r\nFor the year 2009, the most frequent noun-verb pair in titles is “leader karel”. The other most common keywords such as “karel arrested” suggests that in 2009 POK’s leader Elian Karel has been arrested. Other events that have taken place in this year are “gastech adopting new logo”.\r\n\r\n\r\n4.2.5 Timeline: Unfolding of Events across Years\r\nAfter obtaining all of the most frequent keywords from the titles of all articles across all years, the timeline shows that from 1982 to 1994, there have been talks about foreign investment and this could suggest that the Tethys based GasTech company is looking to invest its company in another country and towards the end of that period, they have settled on Kronos which can be derived from “Shores of Kronos” keyword in 1994.\r\nWithin two years of the opening of the company, there have been discussions about possible contamination as seen from keyword in 1997. During the period of 1998 to 2004, there have been many instances where “public health” was in talks and this suggests that the issue on the health due to the contamination by GasTech company was a serious matter and was mentioned in many articles. From 2007 onward, we can see keywords representing protests which represents the POK becoming stronger year after year.\r\n\r\n\r\n4.3 Exploring Relationships\r\n4.3.1 Overall Distribution of Emails\r\nUpon dividing all emails to their respective dates, it is visible that there was more activity on the 14th and 15th of January 2014 compared to any other dates. Among all employment type groups, the activity amongst ‘Information Technology’ seems quite suspicious with very unevenly distributed with 14th Jan having the most number or email transmissions compared to any other day. The ‘Executives’ seem to have little communication with other employment groups. To extract further information,the emails were segregated into work and non-work related as shown below.\r\n\r\n\r\nWork Related Emails\r\nAfter viewing the work related emails, we can see that there is heavy weight assigned to Information Technology and this eliminates any suspicious activity we assumed earlier.\r\nThe employees within the Engineering department seem to have more communication between themselves than with other departments.\r\nThere is one particular Information Technology employee who seems to have communication with the Security and Administration departments but not with the Information Technology department itself which appears to be a bit fishy.\r\nNon-Work Related Emails\r\nThe Executive department has more number of non-work related emails than work related and this can be explained by the thickness of the edges between them.\r\nOne Information Technology employee appears to be the most common receiver and sender of non owrk related emails and this can be extracted by the placement of one blue node at the center.\r\nThere is heavy transmission of unofficial emails among the Administrative department and such activity has been further explored to identify any possible leads to the incident in the coming visualizations.\r\n4.3.2 Deeper Look into Sensitive Emails\r\nAmong all the non-work related emails, after filter the emails with subjects belonging to either Politics, Kronos or Media, some strong insights are extracted. It is clear now that the email exchanges between the executives were mostly those with sensitive content and hence, were not shared with the other employees. Also, it is notable that there is a group of all Kronos nationals belonging to the Security department having discussions on such sensitive content and the employees are “Hennie Osvaldo”, “Inga Ferro”, “Isia Vann”, “Loreto Bodrogi” and “Minke Mies”.\r\n\r\n\r\n4.3.3 Citizenship Biases in Employment Type\r\nThe employees who are Kronos citizens were only offered employment in the administration, facilities and security. All the higher profile jobs such as engineering, information technology and executive were only offered to those who are Tethys citizens.\r\n\r\n\r\n4.3.4 Most Important Employees\r\nOf all emails taht were exchanged in the duration of two weeks, there are 3 employees who are identified to have been the most common entities across all subjects. Among these three, two of them are Tethys citizens whereas the remaining employee if a citizen of Kronos.\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:46:36+08:00"
    },
    {
      "path": "background.html",
      "title": "1.0 Background",
      "description": "",
      "author": [],
      "contents": "\r\n\r\nContents\r\n1.1 Backgroung\r\n1.2 Literature Review of 2014 VAST Challenge1.2.1 Timeline Visualizations\r\n1.2.2 Text Visualization\r\n1.2.3 How Much is Too Much\r\n1.2.4 Network Graph\r\n\r\n\r\n1.1 Backgroung\r\nThis visual investigation is based on VAST Challenge 2021 - The Kronos Incident Mini Challenge 1. The overview of the challenge is as below:\r\n\"In the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.\r\nIn January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.\"\r\nMini-Challenge 1 looks at the relationships and conditions that led up to the kidnapping. As an analyst, you have a set of current and historical news reports at your disposal, as well as resumes of numerous GAStech employees and email headers from two weeks of internal GAStech company email. Can you identify the complex relationships among all of these people and organizations?\r\nQuestions\r\nCharacterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources?\r\nCharacterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples.\r\nGiven the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships.\r\n1.2 Literature Review of 2014 VAST Challenge\r\nThe 2021 IEEE VAST Challenge is a variation on a 2014 year’s challenge with modified data and new questions. So this literature review is based on some of the submissions from that year.\r\n1.2.1 Timeline Visualizations\r\n\r\nFrom the images above, the timeline in figure ‘a’ consists of too much text and appears to be very cluttered. When any graph is built, one must work on showing what is required and what is not. Putting in the whole text for every event makes it difficult for the user to identify the important aspects and also it is not very readable. The graph also seems a little inconsistent as a few of the vertical lines are dotted and some are not. There should have been a legend to show the differences.\r\nComing to timeline shown in figure ‘b’, even thought the plot is not cluttered, the circled numbers of events not being attached to the vertical lines makes it difficult to understand at what time instance the event falls under.\r\n1.2.2 Text Visualization\r\n\r\nIn the figure above ,users are provided with text across various articles to compare. This makes it very difficult for a user to interpret the main aspects across all the articles. Instead of showcasing the whole text in the articles, the user could have been provided an option to select the articles he wants to compare and the visualization of these articles can be in the form of wordcloud which makes it more easy to look at the most important terms used.\r\nAlso if there are more articles to be included to the dataset, then the number of articles shown to the user would also increase making it all the more cluttered then it already is. Therefore, giving the option to the user to select the articles he wants to view and compare with would be a more ideal solution.\r\n1.2.3 How Much is Too Much\r\n\r\nScreen shot ‘a’ of a visualization dashboard shown above consists of 3 different plots that contain too much of data making it difficult to view the wordings. When showcasing multiple plots in one page, it is better to use a subset of the data to visualize because by doing so, the amount of data to visualize would decrease and this makes the plots more readable. One way in which this approach can be achieved by clustering the data and then provide the use an option to select the cluster he wants to visualize.\r\nThe same applies for the second figure ‘b’. Here, the user is given an option to select a user from a dropdow. However, the comparisons performed with the selected user are too many and the plot appears to be very cluttered making it difficult to read the wordings. Similar to what was suggested earlier, considering clustering the data will help reduce the clutter.\r\n1.2.4 Network Graph\r\n\r\nThe network graph above, consists of many nodes and due to the overlap of the node names with the edges, some of the text is difficult to read. An alternat approact would e to divide the network graph into sub graphs. When the graph is divided, it makes the graphs less dense and also enhances the redability.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:46:38+08:00"
    },
    {
      "path": "building_viz.html",
      "title": "3.0 Building the Visualization",
      "author": [],
      "contents": "\r\n\r\nContents\r\n3.1 Visualizing Text Data3.2.1 Text Net\r\n3.2.2 Clustering\r\n3.2.3 Text Plot\r\n3.2.4 Comparison Cloud\r\n3.2.5 Timeline\r\n3.2.6 Correlation Graph\r\n3.2.7 Wordcloud\r\n3.2.8 Bigram\r\n\r\n3.2 Visualizing Network Data3.2.1 Overall Distribution of Emails\r\n3.2.2 Official and Unofficial Relationships\r\n3.2.3 Unofficial Relationships Related to ‘Politics’, ‘Kronos’and ’Media’\r\n3.2.4 Role of Citizenship in Employment Type\r\n3.2.5 Most Important Employees\r\n\r\n\r\n3.1 Visualizing Text Data\r\n3.2.1 Text Net\r\nThe textnet is used to represent relationships between words. The first node set is words found in the newspaper articles, and the second node set is the newspapers themselves. That is, one can create a network where newspapers are connected by their use of the same words.\r\nPackages Required:\r\nThe only R package presently available to implement text network techniques is the textnets package. The most current version of the textnets package is currently available on Github. To install textnets—or any other package hosted on Github— you will need the devtools package. We then install ‘textnets’ package and all the dependent packages that are required to run the ‘textnets’ package.\r\n\r\n\r\nlibrary(devtools)\r\ninstall_github(\"cbail/textnets\")\r\n\r\npackages = c('textnets','dplyr','Matrix','tidytext','stringr','SnowballC','reshape2','igraph','ggraph','networkD3')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPreparing Texts:\r\nThe textnets package requires text that is contained within a dataframe, where each row represents a document. The text of each document must be contained within a single row therefore we have to group all newspaper article based on their name and club all the articles text to one row. Since we have 29 newspapers provided, we should be having 29 rows. For this dataframe, as discussed above only the newspaper name and the content are required. Therefore, we can drop the other columns.\r\n\r\n\r\ncleaned_text=read.csv(\"data/cleanArticles.csv\")\r\ncleaned_text$Published=as.Date(cleaned_text$Published,format=\"%Y-%m-%d\")\r\ntextNet_data=cleaned_text %>%\r\n  group_by(newsgroup) %>%\r\n  summarise_all(funs(toString(na.omit(.))))\r\ntextNet_data=textNet_data[,!(names(textNet_data) %in% c(\"id\",\"Location\",\"Published\",\"Title\"))]\r\n\r\n\r\n\r\nThe PrepText function prepares texts for networks using all types of words.This function requires the user to provide four inputs:\r\na dataframe that meets the requirements described above\r\nthe name of a column within that dataframe containing the texts that the user would like to analyze in character format (specified via the textvar argument)\r\na column within that dataframe describing the groups through which the words of those texts will be linked (specified via the groupvar argument). The groupvar argument is often some type of document identifier or the name of the author of the document (in this case newspapaer name). In network analysis terminology, the textvar and the groupvar are specifying the nodes sets of a two-mode network.\r\nthe PrepText function requires the user to specify which projection of the two-mode network should be created using the node_type argument. If one wishes to build a network where the nodes are words, node_type=words should be specified. If one wishes to build a network where nodes are the authors of documents or any other meta data, then node_type=groups should be used.\r\n\r\n\r\nnews_text_data <- PrepText(textNet_data, textvar=\"Content\", groupvar=\"newsgroup\", node_type = \"groups\",\r\n                           remove_stop_words=TRUE, remove_numbers=TRUE)\r\n\r\n\r\n\r\nOnce the data is converted to the required format, we pass this to a function that reads in an object created using the PrepText function and outputs a weighted adjacency matrix.\r\n\r\n\r\nnews_text_network <- CreateTextnet(news_text_data)\r\n\r\n\r\n\r\nStatic Visualization:\r\nTo visualize text networks created in the previous step we use the ‘VisTextNet’ function to create a network diagram where nodes are colored by their cluster. Text networks will be very dense because most documents share at least one word. To make text networks more readable, the visualize function requires the user to specify a prune_cut argument, which specifies which quantile of edges should be kept for the visualization. We shall set this variable to be .30 meaning that only edges that have a weight in the 30th percentile or above will be kept. The label_degree_cut specifies the degree, or number of each connections, that nodes which are labeled should have.\r\n\r\n\r\nVisTextNet(news_text_network, .30, label_degree_cut=1)\r\n\r\n\r\n\r\n\r\nNote that the node color corresponds to text communities (same color indicates a strong relationship between its components).\r\n3D Visualization:\r\nWe can also output an interactive visualization of the text network, where the user can mouse over each node in order to reveal its node label. Once again, nodes are colored by their cluster class, and the user must specify a prune_cut argument.\r\n\r\n\r\nVisTextNetD3(news_text_network, .30)\r\n\r\n\r\n\r\n{\"x\":{\"links\":{\"source\":[1,0,4,0,3,4,2,5,7,3,1,2,0,6,2,7,8,3,9,10,12,1,10,13,9,11,2,8,11,5,2,4,14,1,5,11,4,16,5,7,9,17,4,16,17,1,6,1,4,0,7,9,11,18,15,15,21,11,17,19,12,17,19,22,11,6,20,22,4,21,15,0,19,12,17,22,23,5,8,15,12,26,15,5,8,20,9,1,19,7,21,13,18],\"target\":[2,4,5,6,6,6,7,8,9,10,11,11,12,12,12,12,12,13,13,13,14,14,14,14,14,14,14,15,15,15,16,16,16,16,16,16,17,17,17,18,18,18,18,18,19,19,20,20,20,20,21,21,21,21,21,22,22,22,22,22,23,23,23,23,24,24,24,24,24,24,24,24,25,25,25,25,25,26,26,26,27,27,27,27,27,28,28,28,28,28,28,28,28],\"colour\":[\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\"]},\"nodes\":{\"name\":[\"All News Today\",\"Athena Speaks\",\"Central Bulletin\",\"Centrum Sentinel\",\"Daily Pegasus\",\"Everyday News\",\"Homeland Illumination\",\"International News\",\"International Times\",\"Kronos Star\",\"Modern Rubicon\",\"News Desk\",\"News Online Today\",\"Tethys News\",\"The Abila Post\",\"The Continent\",\"The Explainer\",\"The General Post\",\"The Guide\",\"The Light of Truth\",\"The Orb\",\"The Truth\",\"The Tulip\",\"The World\",\"The Wrap\",\"Who What News\",\"World Journal\",\"World Source\",\"Worldwise\"],\"group\":[1,3,3,2,1,5,1,6,5,2,2,3,1,2,3,5,3,4,6,4,1,6,4,4,1,4,5,5,6]},\"options\":{\"NodeID\":\"name\",\"Group\":\"group\",\"colourScale\":\"d3.scaleOrdinal(d3.schemeCategory20);\",\"fontSize\":7,\"fontFamily\":\"serif\",\"clickTextSize\":17.5,\"linkDistance\":50,\"linkWidth\":\"function(d) { return Math.sqrt(d.value); }\",\"charge\":-30,\"opacity\":0.6,\"zoom\":false,\"legend\":false,\"arrows\":false,\"nodesize\":false,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":false,\"opacityNoHover\":0,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]}\r\n3.2.2 Clustering\r\nPackages Required:\r\nggwordcloud - This package is used to display the wordcloud\r\n\r\n\r\npackages = c('ggwordcloud')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nViewing Clusters:\r\nFrom the previous visualization of text net, the newsgroups have been divided into clusters based on strong relationship between its components. To classifying the newsgroups based on the clusters obtained, we use the TextCommunities function, which automatically uses the edge weights and determines the number of clusters within a given network. The function outputs a dataframe with the cluster or “modularity” class to which each document has been assigned.\r\n\r\n\r\n# get text communities and plot their constituting words\r\ntext_communities <- TextCommunities(news_text_network)\r\n\r\nggplot(text_communities %>% filter(modularity_class %in% c(1,2,3,4,5,6)), \r\n       aes(label=group, \r\n       color=modularity_class)) +\r\n  geom_text_wordcloud(eccentricity = 1) +\r\n  scale_size_area(max_size = 15) +\r\n  theme_minimal() +  \r\n  ggtitle(\"Segmentation of Newsgroups into Clusters\")+\r\n   theme(plot.title = element_text(hjust = 0.5))+\r\n  facet_wrap(~modularity_class)\r\n\r\n\r\n\r\n\r\nLets store the cluster number information in a ‘cluster’ column in cleaned_text. This will make it easier to refer to clusters for future visualizations.\r\n\r\n\r\ncleaned_text$cluster=text_communities$modularity_class[match(cleaned_text$newsgroup,text_communities$\r\ngroup)]\r\n\r\n\r\n\r\n3.2.3 Text Plot\r\nPackages Required:\r\ntidytext - This package is used to convert text into a format that is visualizable with the use of ‘unnest_tokens’ function.\r\nudpipe - R package provides language-agnostic tokenization, tagging, lemmatization and dependency parsing of raw text, which is an essential part in natural language processing.\r\ntextplot - To plot data as a text plot, we will be needing this package.\r\n\r\n\r\npackages = c('tidytext','udpipe','textplot')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nText Plot for Cluster 1:\r\nThe word cooccurrence graph graph visualizes collections of word pair and how frequent these word pairs occur. We create a cooccurrence data frame where each row contains the word pair and how many they occur. That cooccurrence data frame will be used as the input for the textplot_cooccurrence function.\r\nAfter we get the correct input, now we can create the chart. We will take only the 15 most occur word pairs. Do note that we are only visualizing the content of cluster 1 articles. We can obtain other clusters by editing the filter criteria in the ‘subset()’ function.\r\n\r\n\r\nusenet_words <- cleaned_text %>%\r\n  unnest_tokens(word, Content) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word)\r\n\r\n##modify the cluster here\r\nx <- subset(usenet_words, cluster == 1)\r\nx <- cooccurrence(x, group = \"id\", term = \"word\")\r\n#x\r\ntextplot_cooccurrence(x, top_n = 15, subtitle = \"showing Cluster 1\")\r\n\r\n\r\n\r\n\r\n3.2.4 Comparison Cloud\r\nPackages Required:\r\ntidytext - This package is used to convert text into a format that is visualizable with the use of ‘unnest_tokens’ function.\r\ntm - This packages has built in functions such as removeStopwords.\r\nwordcloud - The comparison cloud is developed using this package.\r\n\r\n\r\npackages = c('tidytext','tm','wordcloud')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPreparing Data:\r\nTo build a wordcloud, we first need to have the proper data. Therefore, we first extract the required data from cleaned_text and filter it based on the criteria “cluster =5”. This values should be changes when we want to visualize other clusters. After we extract the required data, we need to clean it by removing punctuation, digits and stopwords. All of these can be removed by using regular expression as stated in the code chunk below.\r\n\r\n\r\ntitle_data=cleaned_text %>%\r\n  group_by(newsgroup) %>%\r\n  filter(cluster==5)%>%\r\n  summarise_all(funs(toString(na.omit(.))))\r\n\r\ncorpus2=title_data$Title\r\n\r\n#punctuation removal\r\ncorpus2=gsub(pattern=\"\\\\W\",replace=\" \",corpus2)\r\n#digits removal\r\ncorpus2=gsub(pattern=\"\\\\d\",replace=\" \",corpus2)\r\n#stopwords\r\ncorpus2=tolower(corpus2)\r\ncorpus2=removeWords(corpus2,stopwords(\"english\"))\r\n#remove single letters\r\ncorpus2=gsub(pattern=\"\\\\b[A-z]\\\\b{1}\",replace=\" \",corpus2)\r\n#remove white space\r\ncorpus2=stripWhitespace(corpus2)\r\n\r\n\r\n\r\nAfter we have obtained the clean data, we will have convert corpus to Term Document matrix. Terms are words in the corpus as rows and documents as columns. The values of the matrix will be the frequency which states how many time a word appeared in a document. When we print the resultant object, it specifies how sparse the matrix is and the number of terms and documents present. Since cluster 5 has 6 newsgroups, the number of documents must also be 6.\r\n\r\n\r\ncorpus3=Corpus(VectorSource(corpus2))\r\ntdm=TermDocumentMatrix(corpus3)\r\ntdm\r\n\r\n\r\n<<TermDocumentMatrix (terms: 375, documents: 6)>>\r\nNon-/sparse entries: 639/1611\r\nSparsity           : 72%\r\nMaximal term length: 15\r\nWeighting          : term frequency (tf)\r\n\r\nComaprison Cloud for Titles in Cluster 5:\r\nWe will now convert the term document matrix to matrix format and rename the document names to the respective newsgroup. As for the comparison.cloud function, there are multiple parameters that can be set as per requirements. For our purpose, we will set the higher frequency words to be in the center and for it we shall set random.order=FALSE. The maximum number of words that can be displayed is set to 300 and the size and color of the title is set to a suitable value as show below.\r\n\r\n\r\nm=as.matrix(tdm)\r\ncolnames(m)=c(title_data$newsgroup)\r\ncomparison.cloud(m,max.words = 300,random.order=FALSE,colors=brewer.pal(max(3,ncol(m)),\"Dark2\") ,title.size=1,\r\n  title.colors=NULL, match.colors=FALSE,\r\n  title.bg.colors=\"grey90\")\r\n\r\n\r\n\r\n\r\n3.2.5 Timeline\r\nSince the data we have cleaned for the articles also contains the published date, we can make use of the column to visualize how events have been unfolding across the years by means of a timeline.\r\nPackages Required:\r\nggplot2 - Used for plotting graphs\r\nlattice - Used to plot bar chart\r\nknitr - Used to display the dataframes in a proper way\r\n\r\n\r\npackages = c('dplyr','ggplot2','udpipe','lattice','knitr')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nVisualizing Frequency:\r\nPlotting to understand how the frequency of articles published is.\r\n\r\n\r\nnews=cleaned_text\r\nnews %>% group_by(Published) %>% count() %>% ggplot() + geom_line(aes(Published,n, group = 1))+\r\n ggtitle(\"Trend of Articles Published\")+\r\n   theme(plot.title = element_text(hjust = 0.5))+\r\n  ylab(\"Number of Articles\")\r\n\r\n\r\n\r\n\r\nPreparing Data:\r\nBefore we move on to perform text analysis let’s split year from Published date.\r\n\r\n\r\n#Extract only year\r\nnews$year <- format(news$Published, \"%Y\")\r\nyears=sort(unique(news$year))\r\nyears\r\n\r\n\r\n [1] \"1982\" \"1984\" \"1992\" \"1993\" \"1994\" \"1995\" \"1996\" \"1997\" \"1998\"\r\n[10] \"1999\" \"2000\" \"2001\" \"2002\" \"2003\" \"2004\" \"2005\" \"2007\" \"2009\"\r\n[19] \"2010\" \"2011\" \"2012\" \"2013\" \"2014\"\r\n\r\nUdpipe Package provides pretrained language models for respective languages and we can download the required model using udpipe_load_model function. This will automatically convert the text into clean text.\r\n\r\n\r\nmodel <- udpipe_download_model(language = \"english\")\r\nudmodel_english <- udpipe_load_model(model)\r\n\r\n\r\n\r\nFiltering data only for 2009. The udpipe_annotate() function takes the language model and annoates the given text data. This is a function that is within the ‘udpipe’ package. We then convert this object to a dataframe.\r\nTOP NOUN — VERB Pairs as Keyword pairs: In English (or probably in many languages), Simple a noun and a verb can form a phrase. Like, Dog barked — with the noun Dog and Barked, we can understand the context of the sentence. Reverse-engineering the same with this titles data, let us bring out top phrases - that are just keywords/topics\r\n\r\n\r\nnews_more <- news %>% filter(year == 2009)\r\ns <- udpipe_annotate(udmodel_english, news_more$Title)\r\nx <- data.frame(s)\r\n\r\n\r\n## Using a sequence of POS tags (noun phrases / verb phrases)\r\nx$phrase_tag <- as_phrasemachine(x$upos, type = \"upos\")\r\nstats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \r\n                          pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \r\n                          is_regex = TRUE, detailed = FALSE)\r\nstats <- subset(stats, ngram > 1 & freq > 3)\r\nstats$key <- factor(stats$keyword, levels = rev(stats$keyword))\r\nbarchart(key ~ freq, data = head(stats, 20), col = \"magenta\", \r\n         main = \"Keywords - simple noun phrases\", xlab = \"Frequency\")\r\n\r\n\r\n\r\n\r\nWe can re run the above chuck of code by changing the filter criteria “year==” and replace it with all the years we have. And after running the plot each time , we extract the 1st keyword pair which is the highest frequency and store it into a list called ‘keywords’.\r\n\r\n\r\n#keywords=append(keywords,stats$keyword[1])\r\n\r\n\r\n\r\nWhen we run the barplot for all available years and store the 1st keyword, we should be getting the below values.\r\n\r\n\r\nkeywords=list(\"central role\", \"mediterranean sea\",\"foreign investment\",\"foreign investment\",\r\n              \"shores of kronos\",\"grand opening\",\"gas fields\",\"elodis possible contamination\",\r\n              \"public health\",\"minister of health\",\"president kapelou\",\"taxes on oil\",\r\n              \"public health\",\"hank fluss\",\"public health\",NA,\"near elodis\",\"leader karel\",\r\n              \"anniversary of protests\",\"public threat\",\"presidential manor\",\"ipo makes\",\"scene blog\")\r\n\r\n\r\n\r\nNext, we shall creat another list that consists of events that explain the keywords and then we will unlist bothe the keywords and event list and create a dataframe with years,keywords and event.\r\n\r\n\r\nevent=list(\"Before Opening\",\"Before Opening\",\"Before Opening\",\"Before Opening\",\"Before Opening\",\r\n           \"Contamination\",\"Contamination\",\"Contamination\",\"Health  Impact\",\"Health  Impact\",\r\n           \"Health  Impact\",\"Health  Impact\",\"Health  Impact\",\"Health  Impact\",\"Health  Impact\",\r\n           \"Health  Impact\",\"Protests\",\"Protests\",\"Protests\",\"Protests\",\r\n           \"Protests\",\"Protests\",\"Protests\")\r\n\r\nkeywords=unlist(keywords, use.names=FALSE)\r\nevent=unlist(event, use.names=FALSE)\r\n\r\ntimeline=data.frame(years,keywords,event)\r\n\r\n\r\n\r\nThen, we will assign colors for appropriate groupings of all the uniques values in ‘event’ so our events will be color coded by type of keyword.\r\n\r\n\r\n# Add a specified order to these event type labeles\r\nEvent_type_levels <- c(\"Before Opening\",\"Contamination\" ,\"Health  Impact\",\"Protests\")\r\n\r\n# Define the colors for the event types in the specified order\r\nEvent_type_colors <- c(\"#0070C0\", \"#FFC000\",  \"#00B050\", \"#C00000\")\r\n\r\n# Make the Event_type vector a factor using the levels we defined above\r\ntimeline$event <- factor(timeline$event, levels= Event_type_levels, ordered=TRUE)\r\n\r\n\r\n\r\nEach keyword on the timeline will need to be positioned carefully. We will vary the height or direction on the timeline keywords to avoid overlapping or overcrowded text descriptions. Then, we shall merge these variables to the timeline dataframe and call the new dataframe as ‘Merkel’.\r\n\r\n\r\n# Set the heights we will use for our keywords.\r\npositions <- c(0.5, -0.5, 1.0, -1.0, 1.25, -1.25, 1.5, -1.5, 1.75, -1.75) \r\n\r\n# Set the directions we will use for our keyword, for example above and below.\r\ndirections <- c(1, -1) \r\n\r\n\r\n# Assign the positions & directions to each date from those set above.\r\nline_pos <- data.frame(\r\n    \"years\"=unique(timeline$years),\r\n    \"position\"=rep(positions, length.out=length(unique(timeline$years))),\r\n    \"direction\"=rep(directions, length.out=length(unique(timeline$years))))\r\n\r\n\r\n# Create columns with the specified positions and directions for each milestone event\r\nMerkel <- merge(x=timeline, y=line_pos, by=\"years\", all = TRUE) \r\n# Let's view the new columns.\r\nkable(head(Merkel))\r\n\r\n\r\nyears\r\nkeywords\r\nevent\r\nposition\r\ndirection\r\n1982\r\ncentral role\r\nBefore Opening\r\n0.50\r\n1\r\n1984\r\nmediterranean sea\r\nBefore Opening\r\n-0.50\r\n-1\r\n1992\r\nforeign investment\r\nBefore Opening\r\n1.00\r\n1\r\n1993\r\nforeign investment\r\nBefore Opening\r\n-1.00\r\n-1\r\n1994\r\nshores of kronos\r\nBefore Opening\r\n1.25\r\n1\r\n1995\r\ngrand opening\r\nContamination\r\n-1.25\r\n-1\r\n\r\nWe are ready to plot our timeline now!\r\nStep 1: We first start by creating a plan chart consisting of our x and y coordinates and the labels. Then we will set the background to be plain by using the classic theme.\r\n\r\n\r\n# Create timeline coordinates with an x and y axis\r\ntimeline_plot<-ggplot(Merkel,aes(x=years,y= position, col=event, label=Merkel$keywords)) \r\n\r\n# Add the label keywords\r\ntimeline_plot<-timeline_plot+labs(col=\"keywords\") \r\n\r\n# Assigning the colors and order to the keywords\r\ntimeline_plot<-timeline_plot+scale_color_manual(values=Event_type_colors, labels=Event_type_levels, drop = FALSE) \r\n\r\n# Using the classic theme to remove background gray\r\ntimeline_plot<-timeline_plot+theme_classic() \r\n\r\n# Plot a horizontal line at y=0 for the timeline\r\ntimeline_plot<-timeline_plot+geom_hline(yintercept=0, \r\n                color = \"black\", size=0.3)\r\n\r\n# Print plot\r\ntimeline_plot\r\n\r\n\r\n\r\n\r\nStep 2: Now that we have the plain chart ready, we will now add vertical points where the keywords should come in. These lines are placed by making use of the position values we created previously. The legend it set to be at the bottom to enhance the redability.\r\n\r\n\r\n# Plot the vertical lines for our timeline's milestone events\r\ntimeline_plot<-timeline_plot+geom_segment(data=Merkel, \r\n                                          aes(y=Merkel$position,yend=0,xend=Merkel$years), color='black', size=0.2) \r\n\r\n\r\n# Now let's plot the scatter points at the tips of the vertical lines and date\r\ntimeline_plot<-timeline_plot+geom_point(aes(y=Merkel$position), size=3) \r\n\r\n# Let's remove the axis since this is a horizontal timeline and postion the legend to the bottom\r\ntimeline_plot<-timeline_plot+theme(axis.line.y=element_blank(),\r\n                 axis.text.y=element_blank(),\r\n                 axis.title.x=element_blank(),\r\n                 axis.title.y=element_blank(),\r\n                 axis.ticks.y=element_blank(),\r\n                 axis.text.x =element_blank(),\r\n                 axis.ticks.x =element_blank(),\r\n                 axis.line.x =element_blank(),\r\n                 legend.position = \"bottom\"\r\n                ) \r\n# Print plot\r\ntimeline_plot\r\n\r\n\r\n\r\n\r\nStep 3: Next, we will add years to the x axis. We will include two additional years at the start and end of the axis so that all the keywords will be redabe in the chatrt and wont be cut off.\r\n\r\n\r\nyears <- append(years, 1981, 0)\r\nyears <- append(years, 2015)\r\nyears\r\n\r\n\r\n [1] \"1981\" \"1982\" \"1984\" \"1992\" \"1993\" \"1994\" \"1995\" \"1996\" \"1997\"\r\n[10] \"1998\" \"1999\" \"2000\" \"2001\" \"2002\" \"2003\" \"2004\" \"2005\" \"2007\"\r\n[19] \"2009\" \"2010\" \"2011\" \"2012\" \"2013\" \"2014\" \"2015\"\r\n\r\n# Let's add the years\r\ntimeline_plot<-timeline_plot+geom_text(data=as.data.frame(years), \r\n                                       aes(x=years,y=-0.25,label=years, fontface=\"bold\"),size=2.5, color='black') \r\n\r\n\r\n\r\nStep 4: We need to add the labels of each keyword now. To do this we have to define the text position. A clean timeline should have the labels situated a bit above the scatter points. Since we have the positions of the points already defined, we will place the labels 0.2 pts away from the scatter points.\r\n\r\n\r\n# Lets offset the labels 0.2 away from scatter points\r\ntext_offset <- 0.2 \r\n\r\n# Let's use the absolute value since we want to add the text_offset and increase space away from the scatter points \r\nabsolute_value<-(abs(Merkel$position)) \r\ntext_position<- absolute_value + text_offset\r\n\r\n# Let's keep the direction above or below for the labels to match the scatter points\r\nMerkel$text_position<- text_position * Merkel$direction \r\n\r\n# View head of the table\r\nkable(head(Merkel))\r\n\r\n\r\nyears\r\nkeywords\r\nevent\r\nposition\r\ndirection\r\ntext_position\r\n1982\r\ncentral role\r\nBefore Opening\r\n0.50\r\n1\r\n0.70\r\n1984\r\nmediterranean sea\r\nBefore Opening\r\n-0.50\r\n-1\r\n-0.70\r\n1992\r\nforeign investment\r\nBefore Opening\r\n1.00\r\n1\r\n1.20\r\n1993\r\nforeign investment\r\nBefore Opening\r\n-1.00\r\n-1\r\n-1.20\r\n1994\r\nshores of kronos\r\nBefore Opening\r\n1.25\r\n1\r\n1.45\r\n1995\r\ngrand opening\r\nContamination\r\n-1.25\r\n-1\r\n-1.45\r\n\r\nStep 5: For the final step, we can add the labels to the timeline for our keywords and display the timeline.\r\n\r\n\r\ntimeline_plot<-timeline_plot+geom_text(aes(y=Merkel$text_position,label=Merkel$keywords),size=3.5, vjust=0.6)\r\n\r\n# Print plot\r\nprint(timeline_plot)\r\n\r\n\r\n\r\n\r\nThe above timeline plot is extremely useful and easily understandable if we have data based on dates. This plot helps us understand how events have been unfolding through the years, months or dates.\r\n3.2.6 Correlation Graph\r\nPackages Required:\r\nwidyr - ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result. ‘pairwise_cor’ function is from this package.\r\n\r\n\r\npackages = c('widyr')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPreparing Data:\r\nFirst, we need to tidy the text in ‘Content’ column under the ‘cleaned_text’ dataframe. For this, we will make use of the ‘unnest_tokens’ function. Then, we will convert the tidy text into a word-frequenct format.\r\n\r\n\r\nusenet_words <- cleaned_text %>%\r\n  unnest_tokens(word, Content) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word)\r\n\r\nwords_by_newsgroup <- usenet_words %>%\r\n  count(newsgroup, word, sort = TRUE) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nPlotting Graph:\r\nTo identify which newsgroup tend to be closer to another based on words that commonly appear or tend to be most similar because they use certain terms often, we need to rotate the matrix and for each word as a column and obtain the correlation value using ‘pairwise_cor’. By default, pearson method is used to obtain the correlation value.\r\nAfter obtaining the required data format, we can visualize the relationship between newsgroups in network graph. The thickness of the edge is directly proportional to the correlation value. We use graph_from_data_frame() function to convert the data into a graph model. After we have it in graph data model, we make use of ggraph to plot the graph. We can set the correlation filter value to be any value. In the below code chunk, it was set to greater than 0.9 to identify the most highly correlated newsgroups.\r\n\r\n\r\n#corelation calculation\r\nnewsgroup_cors <- words_by_newsgroup %>%\r\n  pairwise_cor(newsgroup, \r\n               word, \r\n               n, \r\n               sort = TRUE)\r\n\r\n#plotting graph\r\nset.seed(123)\r\nnewsgroup_cors %>%\r\n  filter(correlation > 0.9) %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"fr\") +\r\n  geom_edge_link(aes(alpha = correlation, \r\n                     width = correlation)) +\r\n  geom_node_point(size = 3, \r\n                  color = \"lightblue\") +\r\n  geom_node_text(aes(label = name),\r\n                 color = \"red\",\r\n                 repel = TRUE) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\n3.2.7 Wordcloud\r\nPackages Required:\r\nRColorBrewer - Used to modify the color pallet of the word cloud\r\nwordcloud - Used to develop the word cloud\r\n\r\n\r\npackages = c('RColorBrewer','wordcloud')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPlotting the WordClouds:\r\nAfter obtaining the correlation between various newsgroups, we can plot wordclouds to identify the type of content the newsgroups have and see the similary in context used.\r\nBelow is the wordcloud of ‘The World’:\r\n\r\n\r\nset.seed(123)\r\nsub_words1=words_by_newsgroup %>% filter(newsgroup %in% c('The World'))\r\nsub_words2=words_by_newsgroup %>% filter(newsgroup %in% c('Who What News'))\r\n\r\n\r\nwordcloud(words = sub_words1$word \r\n          , freq = sub_words1$n, min.freq = 1,\r\n          max.words=200, random.order=FALSE, rot.per=0.35, \r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nBelow is the wordcloud of ‘Who What News’:\r\n\r\n\r\nwordcloud(words = sub_words2$word \r\n          , freq = sub_words2$n, min.freq = 1,\r\n          max.words=200, random.order=FALSE, rot.per=0.35, \r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\n3.2.8 Bigram\r\nPackages Required:\r\ntidytext - This package is used to convert text into a format that is visualizable with the use of ‘unnest_tokens’ function.\r\ntidyr - the separate function used to separate two words is present in this package.\r\n\r\n\r\npackages = c('tidytext','tidyr')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPreparing Data:\r\nIn the Content column, instead of just trying to see the similarity With other newsgroups, we can also see how frequent do phrases appear together. To look for phrases that mostly appear together, we use the ‘unnest_tokens()’ function by mentioning the token and number of terms we want to link. As discussed earlier, ‘unnest_tokens()’ function is from the tidytext package and it converts clean dataframes into mentioned formats; in this case, the bigram format. We set the token parameter as ‘ngrams’ and the number of terms as 2. By running the below code chunk, there will be a column called ‘bigram’ created containing pair of words that have a higher frequency of appearing together.\r\n\r\n\r\nbigrams <- cleaned_text %>%\r\n  unnest_tokens(bigram, \r\n                Content, \r\n                token = \"ngrams\", \r\n                n = 2)\r\n\r\n\r\n\r\nNext, we will split the two words (since we set n=2, we will have phrases consisting of two words) and remove stop words.\r\n\r\n\r\nbigrams_separated <- bigrams %>%\r\n  filter(bigram != 'NA') %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), \r\n           sep = \" \")\r\nbigrams_filtered <- bigrams_separated %>%\r\n  filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word)\r\n\r\n\r\n\r\nThen, we will calculate the count of each word pair across all rows and store it in the decresing order under the variable ’bigram_counts’bigram_counts.\r\n\r\n\r\nbigram_counts <- bigrams_filtered %>% \r\n  count(word1, word2, sort = TRUE)\r\n\r\n\r\n\r\n*Plotting Graph:\r\nFinally, we will create a graph object by using the graph_from_data_frame() function adn we can also specify the filtering criteria we want. In the code chunk below, the filter is set to plot graph of all phrases that have count more than 40.\r\nWhen plotting the graph, we can include the arrow head to show which word starts and which ends. this makes it more meaningful to understand.\r\n\r\n\r\nbigram_graph <- bigram_counts %>%\r\n  filter(n > 30) %>%\r\n  graph_from_data_frame()\r\n\r\nset.seed(123)\r\na <- grid::arrow(type = \"closed\", \r\n                 length = unit(.15,\r\n                               \"inches\"))\r\nggraph(bigram_graph, \r\n       layout = \"fr\") +\r\n  geom_edge_link(aes(edge_alpha = n), \r\n                 show.legend = FALSE,\r\n                 arrow = a, \r\n                 end_cap = circle(.07,\r\n                                  'inches')) +\r\n  geom_node_point(color = \"lightblue\", \r\n                  size = 5) +\r\n  geom_node_text(aes(label = name), \r\n                 vjust = 1, \r\n                 hjust = 1) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\n3.2 Visualizing Network Data\r\nPackages Required:\r\nThe purpose for requiring the packages are:\r\nlubridate - The ‘wday’ function which converts date to a weekday belongs to this package.\r\ntidygraph - ‘tbl_graph’ function which converts data to an object to display network belons to this package.\r\nggraph - Plotting of the network object happens via this package.\r\n\r\n\r\npackages = c('lubridate','tidygraph','ggraph')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nPreparing Data:\r\nFor easier understanding, lets save our clean email data as “edges” and employee data as “nodes”.\r\n\r\n\r\nedges <- read.csv(\"data/cleanEmail.csv\")\r\nnodes<- read.csv(\"data/cleanEmployee.csv\")\r\n\r\n\r\n\r\nNext, let’s create an aggregated edges that consists of a new variable called “Weight” which holds the count on how many times source sends email to target. for the first aggregate, we shall group by ‘SentDate’. We shall create 3 categories of network graph. One represents all the emails across all employess and the other two will look into only those that are work and non-work related.\r\n\r\n\r\n#all emails\r\nedges_aggregated=edges %>% \r\n  group_by(Source,Target,SentDate) %>%\r\n  summarise(Weight=n()) %>%\r\n  filter(Weight >1) %>%\r\n  ungroup()\r\n\r\n#work related emails only\r\nedges_official=edges %>% \r\n  filter(MainSubject == \"Work related\") %>%\r\n  group_by(Source,Target,SentDate) %>%\r\n  summarise(Weight=n()) %>%\r\n  filter(Weight >1) %>%\r\n  ungroup()\r\n\r\n#non-work related emails only\r\nedges_unofficial=edges %>%\r\n  filter(MainSubject == \"Non-work related\") %>%\r\n  group_by(Source,Target,SentDate) %>%\r\n  summarise(Weight=n()) %>%\r\n  filter(Weight >1) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nNow that we have the aggregated edges, we have to create a wrapper to build the graph model. This can be done using the ‘tbl_graph’ function present in the ‘tidygraph’ package.\r\n\r\n\r\nnetwork_graph = tbl_graph(nodes=nodes, edges=edges_aggregated,\r\n                          directed=TRUE  )\r\nnetwork_graphOfficial = tbl_graph(nodes=nodes, edges=edges_official,\r\n                          directed=TRUE  )\r\nnetwork_graphUnofficial = tbl_graph(nodes=nodes, edges=edges_unofficial,\r\n                          directed=TRUE  )\r\n\r\n\r\n\r\n3.2.1 Overall Distribution of Emails\r\nThe network graphs we created consists of 3 parts 1) layout: the layout you want to use 2) The aesthetics for edges 3) The aesthetics for nodes\r\nFor all these, we make use of the ‘ggraph’ package. We use ‘theme_graph’ to modify the background color. The node and edge color can be changed by specifying the color in the aes() method under the variable ‘color’. We can also add other parameters such as the layout type and also color the nodes based on the employment category so as to understand the connectivity between various employment categories. The thickness of the edges will be mapped with the Weight variable by using the ‘scale_edge_width’ function’. So the thicker the edge, the more emails passed between the two nodes.\r\nDistribution of all email across all employees for the duration of 2 weeks before the incident.\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(network_graph, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n  scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CurrentEmploymentType), \r\n                  size = 2)\r\ng + facet_edges(~SentDate)\r\n\r\n\r\n\r\n\r\n3.2.2 Official and Unofficial Relationships\r\nWork Related Emails:\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(network_graphOfficial, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n    scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CurrentEmploymentType, \r\n                      size = 3))\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\nNon-Work Related Emails:\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(network_graphUnofficial, \r\n            layout = \"nicely\") +\r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n  scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CurrentEmploymentType), \r\n                  size = 3)\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\n3.2.3 Unofficial Relationships Related to ‘Politics’, ‘Kronos’and ’Media’\r\nIdentifying the employees who had discussion about any of the the key terms such as ‘politics’, ‘Kronos’and ’media’ under the non-wok related emails. We create another dataframe called ‘unofficial_discussions’ which is the output of filtering the original ‘edges’ dataframe based on the content in the ‘Subject’ column consists of any of the mentioned key terms. The output datatable consists of all such informal emails that could provides some insight.\r\n\r\n\r\nkeyterms=c('politics','kronos','media')\r\nunofficial_discussions=edges\r\n\r\nfor (i in (1:nrow(unofficial_discussions))){\r\nunofficial_discussions$filter.condition[i] <-ifelse(ifelse(any(str_detect(unofficial_discussions$Subject[i],\r\n                                                                          keyterms))==TRUE,TRUE,FALSE),TRUE,FALSE)\r\n}\r\n\r\nunofficial_discussions=unofficial_discussions %>% \r\n  filter(filter.condition == TRUE)\r\n\r\n\r\n\r\nNext, let’s create an aggregated edges called unofficial that consists of a new variable called “Weight” which holds the count on how many times source sends email to target just like how we did for the above graphs.\r\n\r\n\r\nunofficial=unofficial_discussions %>% \r\n  group_by(Source,Target,SentDate) %>%\r\n  summarise(Weight=n()) %>%\r\n  filter(Weight >1) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nSome additional steps that we need to perform are that if we want to only display the nodes that have edges in the graph, then we need to filter the nodes dataframe to contain only the ‘ids’ that are present in the ‘unofficial’ edge object. Therefore, we obtain the unique ids that are present in the ‘unofficial’ edge object and creat a new ‘sub_node’ consisting of only those ids.\r\nAn important point to node is that the id values on the node must be continuous. Therfore, we need to relabel the id column by creating a new column called ‘new_id’ and then re edit the source and target in the unofficial dataframe.\r\n\r\n\r\nids=unofficial$Source\r\nids=append(ids,unofficial$Target)\r\nids=unique(ids)\r\n\r\n#Create sub node consisting of ids present in unofficial\r\nsub_nodes=nodes %>% filter(id %in% ids)\r\n\r\n#create new column called new_id\r\nsub_nodes$new_id=1:nrow(sub_nodes)\r\n\r\n#re edit the Source and Target ids\r\nunofficial$Source=sub_nodes$new_id[match(unofficial$Source,sub_nodes$id)]\r\nunofficial$Target=sub_nodes$new_id[match(unofficial$Target,sub_nodes$id)]\r\n\r\n#remove the old id column and rename the new_id column\r\nsub_nodes=sub_nodes[,!(names(sub_nodes) %in% c(\"id\"))]\r\nnames(sub_nodes)[names(sub_nodes) == 'new_id'] <- 'id'\r\n\r\n\r\n\r\nNow that we have the everything ready, we have to create a wrapper to build the graph model.\r\n\r\n\r\ngraphUnofficial = tbl_graph(nodes=sub_nodes, edges=unofficial)\r\n\r\n\r\n\r\nNow, we can build multiple graphs with various coloring criteria to identify who has been discussing about such sensitive information.\r\nFirst, we will identify the employee names who were discussing such informal topics.\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(graphUnofficial, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n    scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = FullName, \r\n                      size = 2))\r\n\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\nNext, we will identify the employment type of the above mentioned people.\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(graphUnofficial, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n    scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CurrentEmploymentType, \r\n                      size = 2))\r\n\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\nLastly, we can identify the citizenship of those employees.\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(graphUnofficial, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n    scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CitizenshipCountry, \r\n                      size = 2))\r\n\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\n3.2.4 Role of Citizenship in Employment Type\r\nUsing facet nodes to separate the network based on the the citizenship of the employees.\r\n\r\n\r\nset.seed(123)\r\ng <- ggraph(network_graph, \r\n            layout = \"nicely\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n  scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CurrentEmploymentType), \r\n                  size = 2)\r\ng + facet_nodes(~CitizenshipCountry)+\r\n  th_foreground(foreground = \"grey80\",  \r\n                border = TRUE) +\r\n  theme(legend.position = 'bottom')\r\n\r\n\r\n\r\n\r\n3.2.5 Most Important Employees\r\nCentrality is used to find the relative role played by different nodes. betweenness provides information on how important the node is. The bigger the node size, higher the betweenness centrality.\r\n\r\n\r\nset.seed(123)\r\ng <- network_graph %>%\r\n  ggraph(layout = \"fr\") + \r\n  geom_edge_link(aes(width=Weight), \r\n                 alpha=0.2) +\r\n  scale_edge_width(range = c(0.1, 5)) +\r\n  geom_node_point(aes(colour = CitizenshipCountry, \r\n                      size = centrality_betweenness()))\r\ng + theme_graph()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:48:36+08:00"
    },
    {
      "path": "data_preparation.html",
      "title": "2.0 Data Preparation",
      "author": [],
      "contents": "\r\n\r\nContents\r\n2.1 Installing and Loading Necessary Packages\r\n2.2 Importing Provided Data\r\n2.3 Processing News Articles2.3.1 Initial Cleaning Steps\r\n2.3.2 Processing Title\r\n2.3.3 Processing Location\r\n2.3.4 Processing Published\r\n2.3.5 Processing Content\r\n\r\n2.4 Processing Email and Employee Data2.4.1 Processing Email Data\r\n2.4.2 Processing Employee Data\r\n\r\n2.5 Storing Clean Data into Files\r\n\r\n2.1 Installing and Loading Necessary Packages\r\nAs part of the data processing, we first start with installing the required packages if they are not already installed and calling the libraries. The below chunk of code will handle all the necessary packages required without one needing to install manually. The required packages will only have to be mentioned in the ‘packages’ list. The purpose of requiring the below packages are:\r\ntidyverse - Majority of the data cleaning function such as ‘read_csv’, ‘startWith’, ‘gsub’ etc all belong in this library.\r\nDT - To display the data frame in an interactive manner to the user.\r\nmgsub - This is similar to the use of ‘gsub’ but can specify more than 1 string pattern.\r\nreadxl, xlsx - To read “.xlxs” or “.xls” files and write into them\r\nsplitstackshape - The function ‘cSplit’ that is present in this package is used to split a string into multiple rows.\r\nknitr - Used to display static dataframes in a proper way\r\n\r\n\r\npackages = c('tidyverse','DT','mgsub','readxl','xlsx','splitstackshape','knitr')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Importing Provided Data\r\nThe data provided are a set of current and historical news reports, as well as resumes of numerous GAStech employees and email headers from two weeks of internal GAStech company email. The data in resumes folder has already been processed and is in the ‘EmployeeRecords.xlsx’ file. Therefore, we will be using that file directly.\r\nFile/Folder Name\r\nAbout\r\nNews Articles\r\nFolder containing various newsgroups and news articles within them\r\nresumes\r\nFolder consisting of resumes of employees\r\nEmployeeRecords.xlsx\r\nDetails on employees\r\nemail headers.csv\r\nEmail headers from two weeks\r\nScreen Shot of data folder is as below\r\n\r\n\r\nImporting News Articles \r\nSince there are multiple files in various folders, we specify the root directory which is the “News Articles” and write a function to read all files from folder into a data frame instead of using a for loop which is slower. After reading all files, we will save all the data into a variable called ‘raw_text’.\r\n\r\n\r\n#root folder directory\r\nnews=\"data/News Articles/\"\r\n\r\n#define a function to read all files from folder into a data frame\r\nread_folder=function(infolder){\r\n  tibble(file=dir(infolder,full.names=TRUE))%>%\r\n    mutate(text=map(file,read_lines))%>%\r\n    transmute(id=basename(file),text) %>%\r\n    unnest(text)\r\n}  \r\n#read data\r\nraw_text=tibble(folder=\r\n                  dir(news,\r\n                      full.names=TRUE)) %>%\r\n  mutate(folder_out=map(folder,\r\n                        read_folder)) %>%\r\n  unnest(cols=c(folder_out)) %>%\r\n  transmute(newsgroup=basename(folder),id,text)\r\n\r\n\r\n\r\nImporting Email and Employee data \r\nFor the data already present in either a “.xlsx” or “.csv” file, we can use the existing functions “read_excel” and “read_csv” respectively. These functions will read the data in the files and output them as data frames. “read_excel” function can be used for not only “.xlsx” but also “.xls” files.\r\n\r\n\r\nemployee_data <- read_excel(\"data/EmployeeRecords.xlsx\")\r\nemail_data <- read_csv(\"data/email headers.csv\")\r\n\r\n\r\n\r\n2.3 Processing News Articles\r\nBelow is how the raw_text datatable looks like. It is visible that the data is very dirty and there is a lot of processing that has to be done.\r\n\r\n\r\nkable(head(raw_text,5))\r\n\r\n\r\nnewsgroup\r\nid\r\ntext\r\nAll News Today\r\n121.txt\r\nSOURCE: All News Today\r\nAll News Today\r\n121.txt\r\n\r\nAll News Today\r\n121.txt\r\nTITLE: POK PROTESTS END IN ARRESTS\r\nAll News Today\r\n121.txt\r\n\r\nAll News Today\r\n121.txt\r\n\r\n\r\n2.3.1 Initial Cleaning Steps\r\nLets start by deleting the text that are empty strings and also it is noticeable that text starting with “SOURCE” is of no additional use to us as the content is already available in column ‘newsgroup’. Also, lets maintain the ‘id’ column as a number and remove the ‘.txt’ extension..\r\n\r\n\r\n#delete empty cells\r\nrow_numbers=which(raw_text$text %in% c(\"\",\" \"))\r\nraw_text=raw_text[-c(row_numbers),]\r\n\r\n#remove .txt from id\r\nraw_text$id=gsub(\".txt\",\"\",raw_text$id)\r\n\r\n#remove the SOURCE as it s already there as newsgroup\r\nrow_numbers=which(grepl(\"SOURCE:\",raw_text$text,fixed=TRUE))\r\nraw_text=raw_text[-c(row_numbers),]\r\nhead(raw_text,10)\r\n\r\n\r\n# A tibble: 10 x 3\r\n   newsgroup     id    text                                           \r\n   <chr>         <chr> <chr>                                          \r\n 1 All News Tod~ 121   \"TITLE: POK PROTESTS END IN ARRESTS \"          \r\n 2 All News Tod~ 121   \"PUBLISHED: 2005/04/06\"                        \r\n 3 All News Tod~ 121   \"LOCATION: ELODIS, Kronos \"                    \r\n 4 All News Tod~ 121   \"Fifteen members of the Protectors of Kronos (~\r\n 5 All News Tod~ 121   \"When the federal police began arresting the P~\r\n 6 All News Tod~ 135   \"TITLE: RALLY SCHEDULED IN SUPPORT OF INCREASE~\r\n 7 All News Tod~ 135   \"PUBLISHED: 2012/04/09\"                        \r\n 8 All News Tod~ 135   \"LOCATION: ABILA, Kronos \"                     \r\n 9 All News Tod~ 135   \"Silvia Marek, leader of the Protectors of Kro~\r\n10 All News Tod~ 135   \"\\\"I'm calling for an end to the heinous corru~\r\n\r\n2.3.2 Processing Title\r\nInstead of keeping all text in one column, it is better to have distinct columns for different contents. For example, there is a sub field starting with “TITLE” in the text column and it can be put into a new separate column. For this, we make use of two functions called ‘startsWith’ which checks if a string starts with the mentioned string and ‘gsub’ which replaces a string with another string.\r\n\r\n\r\nraw_text$Title <-ifelse(startsWith(raw_text$text, \"TITLE\"),gsub(\"TITLE: \",\"\",raw_text$text),\"\")\r\n\r\n\r\n\r\nDo note that for text processing, one has to check the resultant datatable to see if there is any further modification required. In this case, after going through the data table, it was observed that file id 33 has misplaced text and this has to be separately handled as below. From the image below, we can see that the content in “Title”, “Published” and Author\" are misplaced\r\n\r\n\r\n\r\n\r\n#after exploring the data, it appears that the content of file 33 is not proper.\r\n#So there is a need to modify it separately\r\nraw_text$Title=ifelse(raw_text$id==\"33\",\r\n                      ifelse(startsWith(raw_text$text, \"PUBLISHED\"),\r\n                             gsub(\"PUBLISHED: \",\"\",raw_text$text),\"\"),\r\n                      raw_text$Title)\r\nkable(head(raw_text,3))\r\n\r\n\r\nnewsgroup\r\nid\r\ntext\r\nTitle\r\nAll News Today\r\n121\r\nTITLE: POK PROTESTS END IN ARRESTS\r\nPOK PROTESTS END IN ARRESTS\r\nAll News Today\r\n121\r\nPUBLISHED: 2005/04/06\r\n\r\nAll News Today\r\n121\r\nLOCATION: ELODIS, Kronos\r\n\r\n\r\nSince only those records that have “TITLE” in their text column have the column Title populated, the below code will populate all records with Title using the id as a matching criteria. The function ‘unique’ is used to identify the unique values present in the datatable. The ‘which’ function returns the row numbers that meet the criteria. Function ‘match’ is used to obtain value from another dataframe based on the match condition. Lastly, we trim any spaces present at the start or end of the string. This is done using the function ‘str_trim’.\r\n\r\n\r\ntitle_sub_dataframe=unique(raw_text[c(\"id\",\"Title\")])\r\nrow_numbers=which(title_sub_dataframe$Title==\"\")\r\ntitle_sub_dataframe=title_sub_dataframe[-c(row_numbers),]\r\nraw_text$Title=title_sub_dataframe$Title[match(raw_text$id,title_sub_dataframe$id)]\r\n\r\n### trim space at start and end if there exists\r\nraw_text$Title=str_trim(raw_text$Title, side = c(\"both\"))\r\nkable(head(raw_text,3))\r\n\r\n\r\nnewsgroup\r\nid\r\ntext\r\nTitle\r\nAll News Today\r\n121\r\nTITLE: POK PROTESTS END IN ARRESTS\r\nPOK PROTESTS END IN ARRESTS\r\nAll News Today\r\n121\r\nPUBLISHED: 2005/04/06\r\nPOK PROTESTS END IN ARRESTS\r\nAll News Today\r\n121\r\nLOCATION: ELODIS, Kronos\r\nPOK PROTESTS END IN ARRESTS\r\n\r\nAfter looking through the dataframe again, noticed that there are a few titles which do the fit the context. One example of such titles is shown in the figure below. The mentioned two titles in the code chunck below are dates and do not fit the ‘Title’ column. Therefore, ‘ifelse’ statement can used to replace the mentioned titles to NA or else keep the title as it is. Such instance are why we have to check the data table after each step of processing the data.\r\n\r\n\r\n\r\n\r\n##looking at the data now, there appears to be some dates and they can be removed\r\nraw_text$Title=ifelse(raw_text$Title %in% c(\"4 of March of 2010\", \"2014/03/26\"),NA,raw_text$Title)\r\n\r\n\r\n\r\n2.3.3 Processing Location\r\nHaving processed ‘Title’, lets move on to ‘Location’. Applying the similar logic used in Title’, run the below chunk of code to create a new column called ‘Location’. Here, ‘mgsub’ function was used and not ‘gsub’ because here we have to check for more than 1 string pattern and it cannot be done with ‘gsub’ hence, ‘mgsub’ function was used for the same purpose.\r\n\r\n\r\nraw_text$Location <-ifelse(startsWith(raw_text$text, \"LOCATION\"),\r\n                           mgsub(raw_text$text, c(\"LOCATION: \", \"LOCATIONS:  \"), \r\n                                 c(\"\", \"\")),\"\")\r\n\r\nlocation_sub_dataframe=unique(raw_text[c(\"id\",\"Location\")])\r\nrow_numbers=which(location_sub_dataframe$Location==\"\")\r\nlocation_sub_dataframe=location_sub_dataframe[-c(row_numbers),]\r\n\r\nraw_text$Location=location_sub_dataframe$Location[match(raw_text$id,location_sub_dataframe$id)]\r\nhead(raw_text,10)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   newsgroup   id    text                Title              Location  \r\n   <chr>       <chr> <chr>               <chr>              <chr>     \r\n 1 All News T~ 121   \"TITLE: POK PROTES~ POK PROTESTS END ~ \"ELODIS, ~\r\n 2 All News T~ 121   \"PUBLISHED: 2005/0~ POK PROTESTS END ~ \"ELODIS, ~\r\n 3 All News T~ 121   \"LOCATION: ELODIS,~ POK PROTESTS END ~ \"ELODIS, ~\r\n 4 All News T~ 121   \"Fifteen members o~ POK PROTESTS END ~ \"ELODIS, ~\r\n 5 All News T~ 121   \"When the federal ~ POK PROTESTS END ~ \"ELODIS, ~\r\n 6 All News T~ 135   \"TITLE: RALLY SCHE~ RALLY SCHEDULED I~ \"ABILA, K~\r\n 7 All News T~ 135   \"PUBLISHED: 2012/0~ RALLY SCHEDULED I~ \"ABILA, K~\r\n 8 All News T~ 135   \"LOCATION: ABILA, ~ RALLY SCHEDULED I~ \"ABILA, K~\r\n 9 All News T~ 135   \"Silvia Marek, lea~ RALLY SCHEDULED I~ \"ABILA, K~\r\n10 All News T~ 135   \"\\\"I'm calling for~ RALLY SCHEDULED I~ \"ABILA, K~\r\n\r\nThe unique locations existing in the datatable are:\r\n\r\n\r\nunique(raw_text$Location)\r\n\r\n\r\n [1] \"ELODIS, Kronos \"                                                        \r\n [2] \"ABILA, Kronos \"                                                         \r\n [3] NA                                                                       \r\n [4] \"Abila, Kronos \"                                                         \r\n [5] \"ABILA, Kronos\"                                                          \r\n [6] \"TITLE: KRONOS POLICE ARREST BLOTTER \"                                   \r\n [7] \"TITLE: OF TEN YEARS \"                                                   \r\n [8] \"ELODIS, Kronos\"                                                         \r\n [9] \"TITLE: ARREST BLOTTER OF THE POLICE FORCE KRONOS \"                      \r\n[10] \"CENTRUM, Tethys \"                                                       \r\n[11] \"TITLE: ELODIS PUBLIC HEALTH FACT SHEET \"                                \r\n[12] \"TITLE: GOVERNMENT STANDS UP ANTI\"                                       \r\n[13] \"TITLE: GRAND OPENING GASTECH\"                                           \r\n[14] \"TITLE: Abila police break up sit\"                                       \r\n[15] \"TITLE: Multi\"                                                           \r\n[16] \"This is the first confirmation that today's events surrounding GAStech \"\r\n[17] \"Kronos will add Tethys which anti\"                                      \r\n[18] \"TITLE: DRYING PAPER OF THE HALTING OF THE POLICE OF KRONOS \"            \r\n[19] \"DAVOS, Switzerland \"                                                    \r\n[20] \"TITLE: The movement of the block\"                                       \r\n[21] \"This Article is the second of  three\"                                   \r\n\r\nBased at the locations available, there is further processing that has to be done. After further exploring, it is noticeable that a few files have “LOCATION: TITLE:…” like the one show inth the image below. This does not make any sense as a Location and therefore, any text starting with “TITLE” under the column location can be replace with NA. Also, the syntax for a valid location is “City, Country” as seen from the unique values above, therefore, replacing all strings with no ‘,’ with NA will remove all other text present. Lastly, maintain the naming convention by keeping the city in capitals and country in a title format meaning only the first letter will be capital and others in lower case.\r\n\r\n\r\n\r\n\r\n## need to further clean location\r\n### replace everything that starts with TITLE to NA\r\nraw_text$Location <-ifelse(startsWith(raw_text$Location, \"TITLE\"),NA,raw_text$Location)\r\n\r\n### replace everything without ',' with NA\r\nraw_text$Location <-ifelse(grepl(\",\",raw_text$Location,fixed=TRUE),raw_text$Location,NA)\r\n\r\n### trim space at end\r\nraw_text$Location=str_trim(raw_text$Location, side = c(\"both\"))\r\n\r\n### Standardize all names\r\nraw_text$Location <-ifelse(!is.na(raw_text$Location),\r\n                           paste0(toupper(substring(raw_text$Location,1,\r\n                                                    gregexpr(pattern=',',\r\n                                                             raw_text$Location)[[1]][1])),\r\n                                  substring(raw_text$Location,\r\n                                            gregexpr(pattern =',',\r\n                                                     raw_text$Location)[[1]][1]+1,)),\r\n                           raw_text$Location)\r\nunique(raw_text$Location)\r\n\r\n\r\n[1] \"ELODIS, Kronos\"     \"ABILA, Kronos\"      NA                  \r\n[4] \"CENTRUM, Tethys\"    \"DAVOS, Switzerland\"\r\n\r\nThe location values now look clean and tidy.\r\n2.3.4 Processing Published\r\nNext, lets process the “Published” column. Applying the similar logic used in Title’ and ‘Location’, run the below chunk of code to create a new column called ‘Published’. Just like in ‘Title’, article number 33 has misplaced content therefore, it need to be handled separately.\r\n\r\n\r\nraw_text$Published <-ifelse(startsWith(raw_text$text, \"PUBLISHED\") | \r\n                              startsWith(raw_text$text, \" PUBLISHED\"),\r\n                            mgsub(raw_text$text, c(\"PUBLISHED: \", \" PUBLISHED:  \"),\r\n                                  c(\"\", \"\")),\r\n                            \"\")\r\n\r\n#after exploring the data, it appears that the content of file 33 is not proper.\r\n#So there is a need to modify it separately\r\nraw_text$Published=ifelse(raw_text$id==\"33\",\r\n                          ifelse(startsWith(raw_text$text, \"AUTHOR\"),\r\n                                 gsub(\"AUTHOR: \",\"\",raw_text$text),\"\"),\r\n                          raw_text$Published)\r\n\r\npublished_sub_dataframe=unique(raw_text[c(\"id\",\"Published\")])\r\nrow_numbers=which(published_sub_dataframe$Published==\"\")\r\npublished_sub_dataframe=published_sub_dataframe[-c(row_numbers),]\r\nraw_text$Published=published_sub_dataframe$Published[match(raw_text$id,published_sub_dataframe$id)]\r\n\r\n### trim space at start and end if there exists\r\nraw_text$Published=str_trim(raw_text$Published, side = c(\"both\"))\r\nkable(head(raw_text,3))\r\n\r\n\r\nnewsgroup\r\nid\r\ntext\r\nTitle\r\nLocation\r\nPublished\r\nAll News Today\r\n121\r\nTITLE: POK PROTESTS END IN ARRESTS\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005/04/06\r\nAll News Today\r\n121\r\nPUBLISHED: 2005/04/06\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005/04/06\r\nAll News Today\r\n121\r\nLOCATION: ELODIS, Kronos\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005/04/06\r\n\r\nThe unique published dates existing in the datatable are:\r\n\r\n\r\nunique(raw_text$Published)\r\n\r\n\r\n  [1] \"2005/04/06\"            \"2012/04/09\"           \r\n  [3] \"1993/02/02\"            \"Petrus Gerhard\"       \r\n  [5] \"1998/05/15\"            \"2004/05/29\"           \r\n  [7] \"2013/06/21\"            \"2001/03/22\"           \r\n  [9] \"1998/11/15\"            \"2009/06/20\"           \r\n [11] \"2007/03/21\"            \"1999/07/08\"           \r\n [13] \"1998/05/16\"            \"2009/03/12\"           \r\n [15] \"2012/06/20\"            \"2014/01/19\"           \r\n [17] \"2012/09/08\"            \"2011/06/21\"           \r\n [19] \"By Haneson Ngohebo\"    \"2014/01/20\"           \r\n [21] \"2009/02/21\"            \"2009/06/21\"           \r\n [23] \"1997/10/17\"            \"2013/12/17\"           \r\n [25] \"2011/11/23\"            \"2000/10/04\"           \r\n [27] \"2000/08/18\"            \"2009/03/14\"           \r\n [29] \"2005/09/26\"            \"2010/06/15\"           \r\n [31] \"2011/05/15\"            \"2007/04/11\"           \r\n [33] \"1998/11/17\"            \"2010/10/04\"           \r\n [35] \"2012/03/10\"            \"2010/06/21\"           \r\n [37] \"2011/07/29\"            \"1997/04/24\"           \r\n [39] \"2012/08/24\"            \"2005/09/25\"           \r\n [41] \"2011/07/28\"            \"1997/04/23\"           \r\n [43] \"1997/10/16\"            \"2012/08/23\"           \r\n [45] \"2010/10/03\"            \"2010/06/14\"           \r\n [47] \"1998/11/16\"            \"2000/10/02\"           \r\n [49] \"2009/02/19\"            \"2000/08/17\"           \r\n [51] \"2009/03/13\"            \"20 January 2014\"      \r\n [53] \"21 January 2014\"       \"2012/11/11\"           \r\n [55] \"2002/05/27\"            \"2001/09/02\"           \r\n [57] \"1998/05/17\"            \"2004/05/31\"           \r\n [59] \"1998/03/21\"            \"2001/03/23\"           \r\n [61] \"2009/03/09\"            \"2014/03/26\"           \r\n [63] \"2012/06/22\"            \"1998/04/27\"           \r\n [65] \"2013/06/22\"            \"2000/06/01\"           \r\n [67] \"1993/02/04\"            \"2005/04/07\"           \r\n [69] \"2012/04/10\"            \"1998/08/21\"           \r\n [71] \"2013/02/24\"            \"1999/11/16\"           \r\n [73] \"2009/02/23\"            \"1982/10/03\"           \r\n [75] \"21 October 2013\"       \"2007/03/19\"           \r\n [77] \"2012/06/21\"            \"2009/06/23\"           \r\n [79] \"2014/01/21\"            \"1984/05/05\"           \r\n [81] \"2013/11/13\"            \"2012/02/22\"           \r\n [83] \"2013/02/09\"            \"19 January 2014\"      \r\n [85] \"22 March 2001\"         \"20 June 2011\"         \r\n [87] \"20 June 2012\"          \"05 April 2005\"        \r\n [89] \"31 May 2000\"           \"20 June 2013\"         \r\n [91] \"19 August 1998\"        \"12 March 2009\"        \r\n [93] \"25 May 2002\"           \"7 September 2012\"     \r\n [95] \"15 May 1998\"           \"9 April 2012\"         \r\n [97] \"14 November 1998\"      \"7 July 1999\"          \r\n [99] \"25 April 1998\"         \"31 August 2001\"       \r\n[101] \"2 February  1993\"      \"20 March 2007\"        \r\n[103] \"19 March 1998\"         \"19 June 2009\"         \r\n[105] \"29 May 2004\"           \"7 March 2009\"         \r\n[107] \"19 June 2010\"          \"2001/08/31\"           \r\n[109] \"1993/01/19\"            \"2013/12/16\"           \r\n[111] \"2013/06/20\"            \"1995/10/11\"           \r\n[113] \"1995/11/21\"            \"2001/03/18\"           \r\n[115] \"2009/02/18\"            \"1996/03/14\"           \r\n[117] \"1999/07/04\"            \"22 February 2013\"     \r\n[119] \"21 January 2014  1405\" \"20 February 2012\"     \r\n[121] \"22 June 2009\"          \"4 October 1982\"       \r\n[123] \"14 November 1999\"      \"12 November 2013\"     \r\n[125] \"22 February 2009\"      \"18 March 2007\"        \r\n[127] \"2 October 1982\"        \"7 February 2013\"      \r\n[129] \"3 May 1984\"            \"18 February 2009\"     \r\n[131] \"15 December 2013\"      \"17 March 2001\"        \r\n[133] \"30 August 2001\"        \"14 June 2001\"         \r\n[135] \"7 March 2012\"          \"18 January 1998\"      \r\n[137] \"13 March 1996\"         \"21 November 1995\"     \r\n[139] \"3 July 1999\"           \"20 June 2010\"         \r\n[141] \"10 October 1995\"       \"21 June 2012\"         \r\n[143] \"27 July 2011\"          \"20 June 2009\"         \r\n[145] \"9 November 1998\"       \"18 January 1993\"      \r\n[147] \"12 March 1993\"         \"6 April 2005\"         \r\n[149] \"2007/04/12\"            \"2000/10/03\"           \r\n[151] \"1997/04/25\"            \"2013/12/18\"           \r\n[153] \"2010/06/22\"            \"2011/11/24\"           \r\n[155] \"12 November 2012\"      \"1999/11/15\"           \r\n[157] \"1996/07/08\"            \"2013/02/08\"           \r\n[159] \"2009/06/22\"            \"2012/08/22\"           \r\n[161] \"1997/10/15\"            \"17 December 2013\"     \r\n[163] \"1984/05/04\"            \"1998/08/20\"           \r\n[165] \"5 February 2012\"       \"1998/01/19\"           \r\n[167] \"2010/06/19\"            \"2013/02/22\"           \r\n[169] \"1995/03/30\"            \"1993/03/13\"           \r\n[171] \"2003/05/17\"            \"1992/12/12\"           \r\n[173] \"2000/08/15\"            \"2009/03/08\"           \r\n[175] \"21 June 2009\"          \"2013/09/03\"           \r\n[177] \"1998/11/10\"            \"2001/09/01\"           \r\n[179] \"1994/09/24\"            \"2012/03/09\"           \r\n[181] \"12 August 2009\"        \"1998/11/14\"           \r\n[183] \"2013/10/22\"            \"1993/02/03\"           \r\n[185] \"2009/05/16\"            \"1993/09/20\"           \r\n[187] \"2002/05/25\"            \"1999/02/19\"           \r\n[189] \"1994/02/18\"            \"2001/06/15\"           \r\n[191] \"2004/05/30\"            \"2000/01/15\"           \r\n[193] \"2012/03/08\"            \"2012/02/21\"           \r\n[195] \"30 June 2013\"          \"1995/11/22\"           \r\n[197] \"2 October 2010\"        \"8 March 2012\"         \r\n[199] \"15 October 1997\"       \"24 September 2005\"    \r\n[201] \"2 October 2000\"        \"22 August 2012\"       \r\n[203] \"14 May 2011\"           \"15 November 1998\"     \r\n[205] \"16 August 2000\"        \"22 November 2011\"     \r\n[207] \"10 April 2007\"         \"23 April 1997\"        \r\n[209] \"19 February 2009\"      \"13June 2010\"          \r\n[211] \"16 December 2013\"      \"2010/03/05\"           \r\n[213] \"2009/02/20\"            \"2011/05/16\"           \r\n[215] \"2000/01/17\"            \"1994/09/25\"           \r\n[217] \"2009/02/22\"            \"2000/08/16\"           \r\n[219] \"1999/02/20\"            \"2013/09/04\"           \r\n[221] \"2010/12/18\"            \"2003/05/19\"           \r\n[223] \"1995/10/12\"            \"1993/09/21\"           \r\n[225] \"2009/05/17\"            \"1992/12/13\"           \r\n[227] \"1995/04/01\"            \"2012/06/23\"           \r\n[229] \"2001/06/16\"            \"1995/11/23\"           \r\n[231] \"1996/03/15\"            \"2005/04/08\"           \r\n[233] \"2011/06/22\"            \"1998/01/20\"           \r\n[235] \"2000/01/16\"            \"1995/03/31\"           \r\n[237] \"2002/05/26\"            \"1999/07/09\"           \r\n[239] \"2007/03/22\"            \"2001/03/24\"           \r\n[241] \"1998/03/20\"            \"2012/11/12\"           \r\n[243] \"1993/01/20\"            \"1993/03/14\"           \r\n[245] \"2010/12/19\"            \"1996/07/09\"           \r\n[247] \"30 March 1995\"         \"15 May 2009\"          \r\n[249] \"15 January 2000\"       \"13 November 1998\"     \r\n[251] \"7 July 1996\"           \"15 August 2000\"       \r\n[253] \"18 February 1999\"      \"17 May 2003\"          \r\n[255] \"11 December 1992\"      \"19 September 1993\"    \r\n[257] \"17 February 1994\"      \"2 September 2013\"     \r\n[259] \"23 September 1994\"     \"1998/04/26\"           \r\n[261] \"2012/04/11\"            \"2000/06/02\"           \r\n[263] \"2003/05/18\"            \"1993/09/19\"           \r\n[265] \"1982/10/04\"            \"2007/03/20\"           \r\n[267] \"October 21, 2013\"      \"1984/05/03\"           \r\n[269] \"1982/10/02\"            \"1999/11/14\"           \r\n[271] \"1999/07/05\"           \r\n\r\nThe date is of many different formats and some are even in text. All of these are processed in the below chunk of code. We first convert all the data that is not in a date format to date.\r\n\r\n\r\n##need further cleaning\r\nraw_text$Published=ifelse(raw_text$Published==\"21 January 2014  1405\",\"21 January 2014\",raw_text$Published)\r\nraw_text$Published=ifelse(raw_text$Published==\"October 21, 2013\",\"21 October 2013\",raw_text$Published)\r\n\r\n\r\n\r\nNext, convert all dates in “21 January 2014” format to “Y/M/D” and for this, we make use of Regular Expression. The ‘as.Date’ function can be used to convert the text into date and then formatting it again using ‘format’ function to obtain “Y/M/D” format.\r\n\r\n\r\n## filtering published dates in \"21 January 2014\" format and converting them to Y/M/D format\r\ndates_to_format=str_extract(raw_text$Published, \"^[0-9]{1,2}\\\\D[a-zA-Z]+\\\\D[0-9]{4}\")\r\ndates_to_format=unique(dates_to_format)\r\ndates_to_format=dates_to_format[!is.na(dates_to_format)]\r\n\r\nsub_dates=unique(raw_text[c(\"id\",\"Published\")]) %>% filter(Published %in% dates_to_format)\r\nsub_dates$Published=as.Date(sub_dates$Published,format=\"%d %B %Y\")\r\nsub_dates$Published=format(sub_dates$Published,\"%Y/%m/%d\")\r\n\r\nraw_text$subdates=sub_dates$Published[match(raw_text$id,sub_dates$id)]\r\n\r\n\r\n\r\nSince there are a few texts present in the Published column, after looking into the files of those id’s, it was noticeable that the date was in the next line as shown in the figure below and therefore, for these texts, the published date should be taken from the next immediate row.\r\n\r\n\r\n\r\n\r\n## words improper\r\ndates_to_format=str_extract(raw_text$Published, c(\"Petrus Gerhard\",\"By Haneson Ngohebo\"))\r\ndates_to_format=unique(dates_to_format)\r\ndates_to_format=dates_to_format[!is.na(dates_to_format)]\r\n\r\nsub_dates2=raw_text %>% filter(Published %in% dates_to_format)\r\nrow_numbers=which(startsWith(sub_dates2$text, \"PUBLISHED\"))\r\nrow_numbers=row_numbers+1\r\nsubset=sub_dates2[row_numbers,]\r\nsub_dates2$Published=subset$text[match(sub_dates2$id,subset$id)]\r\n\r\nraw_text$subdates2=sub_dates2$Published[match(raw_text$id,sub_dates2$id)]\r\n\r\n\r\n\r\nWe then update the ‘Published’ column with the modified dates using the code below.\r\n\r\n\r\nraw_text$Published=ifelse(!is.na(raw_text$subdates),raw_text$subdates,raw_text$Published)\r\nraw_text$Published=ifelse(!is.na(raw_text$subdates2),raw_text$subdates2,raw_text$Published)\r\nunique(raw_text$Published)\r\n\r\n\r\n  [1] \"2005/04/06\"       \"2012/04/09\"       \"1993/02/02\"      \r\n  [4] \"1998/03/20\"       \"1998/05/15\"       \"2004/05/29\"      \r\n  [7] \"2013/06/21\"       \"2001/03/22\"       \"1998/11/15\"      \r\n [10] \"2009/06/20\"       \"2007/03/21\"       \"1999/07/08\"      \r\n [13] \"1998/05/16\"       \"2009/03/12\"       \"2012/06/20\"      \r\n [16] \"2014/01/19\"       \"2012/09/08\"       \"2011/06/21\"      \r\n [19] \"2014/01/20\"       \"2009/02/21\"       \"2009/06/21\"      \r\n [22] \"1997/10/17\"       \"2013/12/17\"       \"2011/11/23\"      \r\n [25] \"2000/10/04\"       \"2000/08/18\"       \"2009/03/14\"      \r\n [28] \"2005/09/26\"       \"2010/06/15\"       \"2011/05/15\"      \r\n [31] \"2007/04/11\"       \"1998/11/17\"       \"2010/10/04\"      \r\n [34] \"2012/03/10\"       \"2010/06/21\"       \"2011/07/29\"      \r\n [37] \"1997/04/24\"       \"2012/08/24\"       \"2005/09/25\"      \r\n [40] \"2011/07/28\"       \"1997/04/23\"       \"1997/10/16\"      \r\n [43] \"2012/08/23\"       \"2010/10/03\"       \"2010/06/14\"      \r\n [46] \"1998/11/16\"       \"2000/10/02\"       \"2009/02/19\"      \r\n [49] \"2000/08/17\"       \"2009/03/13\"       \"2014/01/21\"      \r\n [52] \"2012/11/11\"       \"2002/05/27\"       \"2001/09/02\"      \r\n [55] \"1998/05/17\"       \"2004/05/31\"       \"1998/03/21\"      \r\n [58] \"2001/03/23\"       \"2009/03/09\"       \"2014/03/26\"      \r\n [61] \"2012/06/22\"       \"1998/04/27\"       \"2013/06/22\"      \r\n [64] \"2000/06/01\"       \"1993/02/04\"       \"2005/04/07\"      \r\n [67] \"2012/04/10\"       \"1998/08/21\"       \"2013/02/24\"      \r\n [70] \"1999/11/16\"       \"2009/02/23\"       \"1982/10/03\"      \r\n [73] \"2013/10/21\"       \"2007/03/19\"       \"2012/06/21\"      \r\n [76] \"2009/06/23\"       \"1984/05/05\"       \"2013/11/13\"      \r\n [79] \"2012/02/22\"       \"2013/02/09\"       \"2011/06/20\"      \r\n [82] \"2005/04/05\"       \"2000/05/31\"       \"2013/06/20\"      \r\n [85] \"1998/08/19\"       \"2002/05/25\"       \"2012/09/07\"      \r\n [88] \"1998/11/14\"       \"1999/07/07\"       \"1998/04/25\"      \r\n [91] \"2001/08/31\"       \"2 February  1993\" \"2007/03/20\"      \r\n [94] \"1998/03/19\"       \"2009/06/19\"       \"2009/03/07\"      \r\n [97] \"2010/06/19\"       \"1993/01/19\"       \"2013/12/16\"      \r\n[100] \"1995/10/11\"       \"1995/11/21\"       \"2001/03/18\"      \r\n[103] \"2009/02/18\"       \"1996/03/14\"       \"1999/07/04\"      \r\n[106] \"2013/02/22\"       \"2012/02/20\"       \"2009/06/22\"      \r\n[109] \"1982/10/04\"       \"1999/11/14\"       \"2013/11/12\"      \r\n[112] \"2009/02/22\"       \"2007/03/18\"       \"1982/10/02\"      \r\n[115] \"2013/02/07\"       \"1984/05/03\"       \"2013/12/15\"      \r\n[118] \"2001/03/17\"       \"2001/08/30\"       \"2001/06/14\"      \r\n[121] \"2012/03/07\"       \"1998/01/18\"       \"1996/03/13\"      \r\n[124] \"1999/07/03\"       \"2010/06/20\"       \"1995/10/10\"      \r\n[127] \"2011/07/27\"       \"1998/11/09\"       \"1993/01/18\"      \r\n[130] \"1993/03/12\"       \"2007/04/12\"       \"2000/10/03\"      \r\n[133] \"1997/04/25\"       \"2013/12/18\"       \"2010/06/22\"      \r\n[136] \"2011/11/24\"       \"2012/11/12\"       \"1999/11/15\"      \r\n[139] \"1996/07/08\"       \"2013/02/08\"       \"2012/08/22\"      \r\n[142] \"1997/10/15\"       \"1984/05/04\"       \"1998/08/20\"      \r\n[145] \"2012/02/05\"       \"1998/01/19\"       \"1995/03/30\"      \r\n[148] \"1993/03/13\"       \"2003/05/17\"       \"1992/12/12\"      \r\n[151] \"1998/04/26\"       \"2000/08/15\"       \"2009/03/08\"      \r\n[154] \"2013/09/03\"       \"1998/11/10\"       \"2001/09/01\"      \r\n[157] \"1994/09/24\"       \"2012/03/09\"       \"2009/08/12\"      \r\n[160] \"2013/10/22\"       \"1993/02/03\"       \"2009/05/16\"      \r\n[163] \"1993/09/20\"       \"1999/02/19\"       \"1994/02/18\"      \r\n[166] \"2001/06/15\"       \"2004/05/30\"       \"2000/01/15\"      \r\n[169] \"2012/03/08\"       \"2012/02/21\"       \"2013/06/30\"      \r\n[172] \"1995/11/22\"       \"20 January 2014\"  \"21 January 2014\" \r\n[175] \"2010/10/02\"       \"2005/09/24\"       \"2011/05/14\"      \r\n[178] \"2000/08/16\"       \"2011/11/22\"       \"2007/04/10\"      \r\n[181] \"2010/06/13\"       \"2010/03/05\"       \"2009/02/20\"      \r\n[184] \"2011/05/16\"       \"2000/01/17\"       \"1994/09/25\"      \r\n[187] \"1999/02/20\"       \"2013/09/04\"       \"2010/12/18\"      \r\n[190] \"2003/05/19\"       \"1995/10/12\"       \"1993/09/21\"      \r\n[193] \"2009/05/17\"       \"1992/12/13\"       \"1995/04/01\"      \r\n[196] \"2012/06/23\"       \"2001/06/16\"       \"1995/11/23\"      \r\n[199] \"1996/03/15\"       \"2005/04/08\"       \"2011/06/22\"      \r\n[202] \"1998/01/20\"       \"2000/01/16\"       \"1995/03/31\"      \r\n[205] \"2002/05/26\"       \"1999/07/09\"       \"2007/03/22\"      \r\n[208] \"2001/03/24\"       \"1993/01/20\"       \"1993/03/14\"      \r\n[211] \"2010/12/19\"       \"1996/07/09\"       \"2009/05/15\"      \r\n[214] \"1998/11/13\"       \"1996/07/07\"       \"1999/02/18\"      \r\n[217] \"1992/12/11\"       \"1993/09/19\"       \"1994/02/17\"      \r\n[220] \"2013/09/02\"       \"1994/09/23\"       \"2012/04/11\"      \r\n[223] \"2000/06/02\"       \"2003/05/18\"       \"1999/07/05\"      \r\n\r\nThere seem to be still other formats of dates present. The below code will process them into the date formats like other dates.\r\n\r\n\r\ndates_to_format=str_extract(raw_text$Published, \"^[0-9]{1,2}\\\\D[a-zA-Z]+\\\\D{1,2}[0-9]{4}\")\r\ndates_to_format=unique(dates_to_format)\r\ndates_to_format=dates_to_format[!is.na(dates_to_format)]\r\n\r\nsub_dates3=unique(raw_text[c(\"id\",\"Published\")]) %>% filter(Published %in% dates_to_format)\r\nsub_dates3$Published=as.Date(sub_dates3$Published,format=\"%d %B %Y\")\r\nsub_dates3$Published=format(sub_dates3$Published,\"%Y/%m/%d\")\r\n\r\nraw_text$subdates3=sub_dates3$Published[match(raw_text$id,sub_dates3$id)]\r\n\r\n\r\n\r\nNow we will update the Publish column again and remove all other temporary column we create as part of processing. Also, since all the published dates are in the date format, we will chnage the datatype from character to date for the ’Published\" column.\r\n\r\n\r\nraw_text$Published=ifelse(!is.na(raw_text$subdates3),raw_text$subdates3,raw_text$Published)\r\nraw_text=raw_text[,!(names(raw_text) %in% c(\"subdates\",\"subdates2\",\"subdates3\"))]\r\n\r\nraw_text$Published=as.Date(raw_text$Published,format=\"%Y/%m/%d\")\r\nkable(head(raw_text,3))\r\n\r\n\r\nnewsgroup\r\nid\r\ntext\r\nTitle\r\nLocation\r\nPublished\r\nAll News Today\r\n121\r\nTITLE: POK PROTESTS END IN ARRESTS\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005-04-06\r\nAll News Today\r\n121\r\nPUBLISHED: 2005/04/06\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005-04-06\r\nAll News Today\r\n121\r\nLOCATION: ELODIS, Kronos\r\nPOK PROTESTS END IN ARRESTS\r\nELODIS, Kronos\r\n2005-04-06\r\n\r\n2.3.5 Processing Content\r\nSince we have extracted “TITLE”, “LOCATION” and ’PUBLISHED\" from the text column and created separate columns for them ,we can exclude all the records that have the “text” column consisting of these content.\r\n\r\n\r\n# removing text with TITLE:, LOCATION:, PUBLISHED:\r\nrow_numbers1=which(startsWith(raw_text$text, \"TITLE\"))\r\nrow_numbers2=which(startsWith(raw_text$text, \"PUBLISHED\"))\r\nrow_numbers3=which(startsWith(raw_text$text, \" PUBLISHED\"))\r\nrow_numbers4=which(startsWith(raw_text$text, \"LOCATION\"))\r\nrow_numbers5=which(startsWith(raw_text$text, \"AUTHOR\"))\r\nraw_text=raw_text[-c(row_numbers1,row_numbers2,row_numbers3,row_numbers4,row_numbers5),]\r\nraw_text\r\n\r\n\r\n# A tibble: 3,015 x 6\r\n   newsgroup  id    text            Title          Location Published \r\n   <chr>      <chr> <chr>           <chr>          <chr>    <date>    \r\n 1 All News ~ 121   \"Fifteen membe~ POK PROTESTS ~ ELODIS,~ 2005-04-06\r\n 2 All News ~ 121   \"When the fede~ POK PROTESTS ~ ELODIS,~ 2005-04-06\r\n 3 All News ~ 135   \"Silvia Marek,~ RALLY SCHEDUL~ ABILA, ~ 2012-04-09\r\n 4 All News ~ 135   \"\\\"I'm calling~ RALLY SCHEDUL~ ABILA, ~ 2012-04-09\r\n 5 All News ~ 152   \"In a glitzy p~ LACK OF DETAI~ ABILA, ~ 1993-02-02\r\n 6 All News ~ 154   \"1998/03/20\"    ELODIS, KRONO~ ELODIS,~ 1998-03-20\r\n 7 All News ~ 154   \"NOTE: This ar~ ELODIS, KRONO~ ELODIS,~ 1998-03-20\r\n 8 All News ~ 154   \"This article ~ ELODIS, KRONO~ ELODIS,~ 1998-03-20\r\n 9 All News ~ 154   \"ELODIS, Krono~ ELODIS, KRONO~ ELODIS,~ 1998-03-20\r\n10 All News ~ 154   \"Two weeks pri~ ELODIS, KRONO~ ELODIS,~ 1998-03-20\r\n# ... with 3,005 more rows\r\n\r\nGoing through the text column, it was visible that there are dates present in some records and this is due to some files having Published date in the next line which was discussed under “Processing Published” category. These dates are present in “yyyy/mm/dd” format or “date month year” format. The below regular expressions were used to detect those patterns in the text column.\r\n\r\n\r\nraw_text1=str_extract(raw_text$text,c(\"^[0-9]{1,2}\\\\D[a-zA-Z]+\\\\D[0-9]{4}\",\"^[0-9]{4}\\\\D[0-9]{1,2}\\\\D[0-9]{1,2}\"))\r\nraw_text1=unique(raw_text1)\r\nraw_text1=raw_text1[!is.na(raw_text1)]\r\nraw_text1[6] =\"1998/05/15\"\r\nraw_text1[7] =\"17 January 1995\"\r\nrow_numbers6=which(raw_text$text %in% raw_text1)\r\nraw_text=raw_text[-c(row_numbers6),]\r\n\r\n\r\n\r\nNow, we have to combine all the records with the same id. After processing, we now will have 845 records that reflect the 845 files that we have and each record consists of the “newsgroup”, “id”, “Title”, “Location”, “Published” and “Content”.\r\n\r\n\r\ncontent=raw_text %>%\r\n  group_by(id) %>%\r\n  summarise_all(funs(toString(na.omit(.))))\r\n\r\nraw_text=raw_text[,!(names(raw_text) %in% c(\"text\"))]\r\nraw_text$Content=content$text[match(raw_text$id,content$id)]\r\ncleaned_text=unique(raw_text)\r\ncleaned_text\r\n\r\n\r\n# A tibble: 845 x 6\r\n   newsgroup  id    Title          Location Published  Content        \r\n   <chr>      <chr> <chr>          <chr>    <date>     <chr>          \r\n 1 All News ~ 121   POK PROTESTS ~ ELODIS,~ 2005-04-06 \"Fifteen membe~\r\n 2 All News ~ 135   RALLY SCHEDUL~ ABILA, ~ 2012-04-09 \"Silvia Marek,~\r\n 3 All News ~ 152   LACK OF DETAI~ ABILA, ~ 1993-02-02 \"In a glitzy p~\r\n 4 All News ~ 154   ELODIS, KRONO~ ELODIS,~ 1998-03-20 \"NOTE: This ar~\r\n 5 All News ~ 237   ELODIS, KRONO~ <NA>     1998-05-15 \"NOTE: This ar~\r\n 6 All News ~ 251   ELODIS PUBLIC~ ELODIS,~ 2004-05-29 \"The Elodis Co~\r\n 7 All News ~ 341   WHO BRINGS A ~ ABILA, ~ 2013-06-21 \"ABILA, Kronos~\r\n 8 All News ~ 391   TAX MEASURE D~ ABILA, ~ 2001-03-22 \"A measure to ~\r\n 9 All News ~ 420   POK REPRESENT~ ABILA, ~ 1998-11-15 \"Representativ~\r\n10 All News ~ 554   ELIAN KAREL D~ ABILA, ~ 2009-06-20 \"Elian Karel, ~\r\n# ... with 835 more rows\r\n\r\n2.4 Processing Email and Employee Data\r\n2.4.1 Processing Email Data\r\nConsidering that the data is based on email, we can make use of the networking graph for visualization and for that purpose, we will need the data to be in the “source” and “target” format. The email_data dataframe currently is very raw and needs to be processed to the required format.\r\n\r\n\r\nkable(head(email_data,2))\r\n\r\n\r\nFrom\r\nTo\r\nDate\r\nSubject\r\nVarja.Lagos@gastech.com.kronos\r\nVarja.Lagos@gastech.com.kronos, Hennie.Osvaldo@gastech.com.kronos, Loreto.Bodrogi@gastech.com.kronos, Inga.Ferro@gastech.com.kronos\r\n1/6/2014 10:28\r\nPatrol schedule changes\r\nBrand.Tempestad@gastech.com.kronos\r\nBirgitta.Frente@gastech.com.kronos, Lars.Azada@gastech.com.kronos, Felix.Balas@gastech.com.kronos\r\n1/6/2014 10:35\r\nWellhead flow rate data\r\n\r\nThe “To” column has various email id’s that are seperated by a ‘,’. We first need to split all the id’s by ‘,’ and then use the ‘cSplit’ function to split all the emails in “To” into multiple rows.\r\n\r\n\r\n# break on , in \"To\"\r\nemail_data_clean <- cSplit(email_data,splitCols= \"To\", sep=\",\", direction=\"long\")\r\nglimpse(email_data_clean)\r\n\r\n\r\nRows: 8,990\r\nColumns: 4\r\n$ From    <chr> \"Varja.Lagos@gastech.com.kronos\", \"Varja.Lagos@gaste~\r\n$ To      <fct> Varja.Lagos@gastech.com.kronos, Hennie.Osvaldo@gaste~\r\n$ Date    <chr> \"1/6/2014 10:28\", \"1/6/2014 10:28\", \"1/6/2014 10:28\"~\r\n$ Subject <chr> \"Patrol schedule changes\", \"Patrol schedule changes\"~\r\n\r\nNext, we shall remove the email id’s where the “From” and “To” are the same. This is to ensure that when be plot the networking graph, we will not have the same user who send an email to himself.\r\n\r\n\r\n# removing same from and to\r\nrow_numbers=which(email_data_clean$From ==email_data_clean$To)\r\nemail_data_clean=email_data_clean[-c(row_numbers),]\r\n\r\n\r\n\r\nSince both the date and the time are displayed in the same column, we shall separate these two and change their datatype accordingly.\r\n\r\n\r\n# separating time and date\r\nemail_data_clean <- cSplit(email_data_clean, splitCols=\"Date\",sep= \" \")\r\n\r\n#changing type of date 1,2\r\nemail_data_clean$Date_1=as.Date(email_data_clean$Date_1,format=\"%m/%d/%Y\")\r\nemail_data_clean$Date_2=format(strptime(email_data_clean$Date_2, format=\"%H:%M\"), format = \"%H:%M\")\r\n\r\nglimpse(email_data_clean)\r\n\r\n\r\nRows: 8,185\r\nColumns: 5\r\n$ From    <chr> \"Varja.Lagos@gastech.com.kronos\", \"Varja.Lagos@gaste~\r\n$ To      <fct> Hennie.Osvaldo@gastech.com.kronos, Loreto.Bodrogi@ga~\r\n$ Subject <chr> \"Patrol schedule changes\", \"Patrol schedule changes\"~\r\n$ Date_1  <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 201~\r\n$ Date_2  <chr> \"10:28\", \"10:28\", \"10:28\", \"10:35\", \"10:35\", \"10:35\"~\r\n\r\nThe whole email id is not required. Therefore, we will remove everything after ‘@’ from both the “From” and “To” columns. For this, we use the regular expression \"@.*\" which specifies that everything from “@” and we use “gsub” function to replace that with and empty string.\r\n\r\n\r\n# remove everything after @\r\nemail_data_clean$From=gsub(\"@.*\",\"\",email_data_clean$From)\r\nemail_data_clean$To=gsub(\"@.*\",\"\",email_data_clean$To)\r\n\r\n\r\n\r\nRename the column names into meaningful names using the “colnames” function.\r\n\r\n\r\n# rename col names\r\ncolnames(email_data_clean) <- c(\"Source.Label\", \"Target.Label\",\"Subject\",\"SentDate\",\"SentTime\")\r\n\r\n\r\n\r\n2.4.2 Processing Employee Data\r\nLets have a look at hoe the employee data is like.\r\n\r\n\r\nglimpse(employee_data)\r\n\r\n\r\nRows: 54\r\nColumns: 18\r\n$ LastName                   <chr> \"Bramar\", \"Ribera\", \"Pantanal\", \"~\r\n$ FirstName                  <chr> \"Mat\", \"Anda\", \"Rachel\", \"Linda\",~\r\n$ BirthDate                  <dttm> 1981-12-19, 1975-11-17, 1984-08-~\r\n$ BirthCountry               <chr> \"Tethys\", \"Tethys\", \"Tethys\", \"Te~\r\n$ Gender                     <chr> \"Male\", \"Female\", \"Female\", \"Fema~\r\n$ CitizenshipCountry         <chr> \"Tethys\", \"Tethys\", \"Tethys\", \"Te~\r\n$ CitizenshipBasis           <chr> \"BirthNation\", \"BirthNation\", \"Bi~\r\n$ CitizenshipStartDate       <dttm> 1981-12-19, 1975-11-17, 1984-08-~\r\n$ PassportCountry            <chr> \"Tethys\", \"Tethys\", \"Tethys\", \"Te~\r\n$ PassportIssueDate          <dttm> 2007-12-12, 2009-06-15, 2013-06-~\r\n$ PassportExpirationDate     <dttm> 2017-12-11, 2019-06-14, 2023-06-~\r\n$ CurrentEmploymentType      <chr> \"Administration\", \"Administration~\r\n$ CurrentEmploymentTitle     <chr> \"Assistant to CEO\", \"Assistant to~\r\n$ CurrentEmploymentStartDate <dttm> 2005-07-01, 2009-10-30, 2013-10-~\r\n$ EmailAddress               <chr> \"Mat.Bramar@gastech.com.kronos\", ~\r\n$ MilitaryServiceBranch      <chr> NA, NA, NA, NA, \"ArmedForcesOfKro~\r\n$ MilitaryDischargeType      <chr> NA, NA, NA, NA, \"HonorableDischar~\r\n$ MilitaryDischargeDate      <dttm> NA, NA, NA, NA, 1984-10-01, 2001~\r\n\r\nRemove unnecessary columns.\r\n\r\n\r\nemployee_data=employee_data[,!(names(employee_data) %in% c(\"CitizenshipBasis\",\"PassportCountry\",\"BirthDate\",\r\n\"CitizenshipStartDate\",\"BirthCountry\",\"PassportIssueDate\",\"PassportExpirationDate\"))]\r\n\r\n\r\n\r\nSince there are no ID’s allotted for the employees, we can create a new column called “id” and populate this column with ID numbers starting from 1 to the number of rows present in the data frame. Since the “Source.Label” and “Target.Label” in the email_data are in firstname.lastname format, we will make use of the “FirstName” , “LastName” columns present in employee_data and use the “paste0” function to join then together with a “.” between them. Pay special attention to ‘Ruscella.Mies Haber’ as there exists two words in her last name so replace the space with a “.”.\r\n\r\n\r\n# create id column\r\nemployee_data$id=1:nrow(employee_data)\r\n\r\nemployee_data$FullName=paste0(employee_data$FirstName,\".\",employee_data$LastName)\r\nemployee_data$FullName=ifelse(employee_data$FullName==\"Ruscella.Mies Haber\",\r\n                              sub(\" \",\".\",employee_data$FullName),\r\n                              employee_data$FullName)\r\n\r\n\r\n\r\nNow that we have two clean data frame, we must map them together using the “match” function based.\r\n\r\n\r\n### mapping two df's\r\nemail_data_clean$Source=employee_data$id[match(email_data_clean$Source.Label,employee_data$FullName)]\r\nemail_data_clean$Target=employee_data$id[match(email_data_clean$Target.Label,employee_data$FullName)]\r\n\r\n\r\n\r\nRemove the “.” from all names to enhanse the redability to the user.\r\n\r\n\r\n## remove \".\" from labels to make it look better\r\nemail_data_clean$Source.Label=sub(\"[.]\",\" \",email_data_clean$Source.Label)\r\nemail_data_clean$Target.Label=gsub(\"[.]\",\" \",email_data_clean$Target.Label)\r\nemployee_data$FullName=gsub(\"[.]\",\" \",employee_data$FullName)\r\nglimpse(email_data_clean)\r\n\r\n\r\nRows: 8,185\r\nColumns: 7\r\n$ Source.Label <chr> \"Varja Lagos\", \"Varja Lagos\", \"Varja Lagos\", \"B~\r\n$ Target.Label <chr> \"Hennie Osvaldo\", \"Loreto Bodrogi\", \"Inga Ferro~\r\n$ Subject      <chr> \"Patrol schedule changes\", \"Patrol schedule cha~\r\n$ SentDate     <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06~\r\n$ SentTime     <chr> \"10:28\", \"10:28\", \"10:28\", \"10:35\", \"10:35\", \"1~\r\n$ Source       <int> 45, 45, 45, 13, 13, 13, 41, 40, 39, 39, 39, 39,~\r\n$ Target       <int> 48, 52, 54, 17, 14, 15, 40, 41, 3, 14, 15, 9, 1~\r\n\r\nCategorizing email based on work related and non work related. Go through the data to identify the subject text for non work related emails. Using that text, we can categorize the records.\r\n\r\n\r\nemail_data_clean$Subject=tolower(email_data_clean$Subject)\r\n\r\nnonWork=c('birthdays','plants','night','concert','coffee','sick','dress','post','funy',\r\n    'lunch','babysitting','politics','cute','parking','vacation','funny','missing',\r\n    'volunteers','nearby','club','investment','found','flowers',\r\n    'defenders','battlestations','article','ha ha','media','retirement')\r\n\r\nfor (i in (1:nrow(email_data_clean))){\r\nemail_data_clean$MainSubject[i] <-ifelse(ifelse(any(str_detect(email_data_clean$Subject[i],\r\n                                          nonWork))==TRUE,TRUE,FALSE),\r\n                                         \"Non-work related\",\"Work related\")\r\n}\r\nglimpse(email_data_clean)\r\n\r\n\r\nRows: 8,185\r\nColumns: 8\r\n$ Source.Label <chr> \"Varja Lagos\", \"Varja Lagos\", \"Varja Lagos\", \"B~\r\n$ Target.Label <chr> \"Hennie Osvaldo\", \"Loreto Bodrogi\", \"Inga Ferro~\r\n$ Subject      <chr> \"patrol schedule changes\", \"patrol schedule cha~\r\n$ SentDate     <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06~\r\n$ SentTime     <chr> \"10:28\", \"10:28\", \"10:28\", \"10:35\", \"10:35\", \"1~\r\n$ Source       <int> 45, 45, 45, 13, 13, 13, 41, 40, 39, 39, 39, 39,~\r\n$ Target       <int> 48, 52, 54, 17, 14, 15, 40, 41, 3, 14, 15, 9, 1~\r\n$ MainSubject  <chr> \"Work related\", \"Work related\", \"Work related\",~\r\n\r\n2.5 Storing Clean Data into Files\r\nHaving cleaned all the data we have, we can now store this cleaned data into files so that it can be used for visualization. When writing into files, we need to set “row.names=FALSE” we avoid writing the row numbers into the first column.\r\n\r\n\r\nwrite.csv(cleaned_text,\"data/cleanArticles.csv\",row.names=FALSE)\r\nwrite.csv(employee_data,\"data/cleanEmployee.csv\",row.names = FALSE)\r\nwrite.csv(email_data_clean,\"data/cleanEmail.csv\",row.names = FALSE)\r\n\r\n\r\n\r\nNote: The dataframes we obtained through the processing steps are now clean and these frames can further be modified according to the need of visualization.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:48:56+08:00"
    },
    {
      "path": "index.html",
      "title": "Visual Investigation of Kidnappings",
      "description": "Does visual analytics help in investigations? Read on to find out! :D\n",
      "author": [],
      "contents": "\r\nWelcome to the home page of my website on VAST Challenge Mini Challenge 1. This challenge is attempted as a part of the coursework of Visual Analytics and Applications, taught by Professor Dr. Kam Tin Seong at Singapore Management University.\r\nAUTHOR: Nikitha Banda\r\nAFFILIATION: School of Computing and Information Systems, Singapore Management University\r\nPUBLISHED: July 24, 2021\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:48:58+08:00"
    },
    {
      "path": "storyboard.html",
      "title": "5.0 Storyboard for R-Shiny",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPrototypeTimeline\r\nNetwork Graph\r\nText Analysis\r\n\r\n\r\nWhy RShiny when we already have the visualizations?\r\nIn our attempt to investigate, if we wanted to examine cluster 2 instead of cluster 1, we had to change the code by editing the filter criteria. This is not user friendly and to overcome this, we make use of a interactive user interface where the user can select any criteria he wants without having to edit the code. This UI is called RShiny.\r\nPrototype\r\nThere will be three tabs in the Shiny app on Kidnapping Investigations: Timeline, Network Graph and Text Analysis. Do note that the visualization ideas are not limited to the screen shots below and they are only for visualization purpose.\r\nTimeline\r\nTo enhance user experience, users would be given the feasibility to select the range of years or the dates of the year specific to 2014 to drill down on the sequence of events that took place in 2014. After user has selected the variables, the timeline will show the important events that unfolded in the selected years and also a bar plot the frequent of keywords will be displayed.\r\n\r\n\r\nNetwork Graph\r\nFor the network visualization, users can specify if they want to look into work related or non work related by selecting the appropriate check box. Apart from this, they can also narrow down on the number of nodes by selecting the nodes the user wants to see.\r\n\r\n\r\nText Analysis\r\nSince a major component of this investigation is textual, there will be a collective of visualizations shown to the user to compare the text. The user can select one or more clusters from the drop down and also specific newsgroups he wants to compare.\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-25T17:48:59+08:00"
    }
  ],
  "collections": []
}
